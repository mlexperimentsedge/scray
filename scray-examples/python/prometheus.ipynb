{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import time\n",
    "import datetime as dt\n",
    "\n",
    "def adddatecolumns(data,pf,column) :\n",
    "    data['year'] = pf[column].apply(lambda x: x.date().year)\n",
    "    data['month'] = pf[column].apply(lambda x: x.date().month)\n",
    "    data['day'] = pf[column].apply(lambda x: x.date().day)\n",
    "    data['hour'] = pf[column].apply(lambda x: x.time().hour)\n",
    "    data['minute'] = pf[column].apply(lambda x: x.time().minute)\n",
    "    #data['second'] = pf[column].apply(lambda x: x.time().second)\n",
    "    #data['microsecond'] = pf[column].apply(lambda x: x.time().microsecond)\n",
    "\n",
    "def converttimestampcolumnn(pf,tsc) :\n",
    "    pf[tsc] = pf[tsc].apply(lambda x: dt.datetime.fromtimestamp(float(x) / 1e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "## Setup charts\n",
    "import pandas as pd\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "print(\"Setup Complete\")\n",
    "\n",
    "#vis functions\n",
    "\n",
    "def label(graph,skip,rot) :\n",
    "    for ind, label in enumerate(graph.get_xticklabels()):\n",
    "        if ind % skip == 0:  # every 10th label is kept\n",
    "            label.set_visible(True)\n",
    "            label.set_rotation(rot)\n",
    "        else:\n",
    "            label.set_visible(False)\n",
    "            \n",
    "def abc(mdcountsall) :\n",
    "    a = mdcountsall.index.get_level_values(0).astype(str)\n",
    "    b = mdcountsall.index.get_level_values(1).astype(str)\n",
    "    c = mdcountsall.index.get_level_values(2).astype(str)\n",
    "    return a + \"-\" + b + \"-\" + c \n",
    "\n",
    "def abcd(mdcountsall) :\n",
    "    return abc(mdcountsall) + \"-\" + mdcountsall.index.get_level_values(3).astype(str)\n",
    "\n",
    "def ymdh(mdcountsall) :\n",
    "    return abcd(mdcountsall)\n",
    "\n",
    "def get_ymdh(mdcountsall) :\n",
    "    a = mdcountsall.index.get_level_values(0).astype(str)\n",
    "    b = mdcountsall.index.get_level_values(1).astype(str)\n",
    "    c = mdcountsall.index.get_level_values(2).astype(str)\n",
    "    d = mdcountsall.index.get_level_values(3).astype(str)\n",
    "    return a,b,c,d\n",
    "\n",
    "def get_ymdh_string(a,b,c,d) :\n",
    "    return a + \"-\" + b + \"-\" + c + \"-\" + d\n",
    "\n",
    "def get_ym(mdcountsall) :\n",
    "    a = mdcountsall.index.get_level_values(0).astype(str)\n",
    "    b = mdcountsall.index.get_level_values(1).astype(str)\n",
    "    return a,b\n",
    "\n",
    "def get_ym_string(a,b) :\n",
    "    return a + \"-\" + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#mdcountsall=pfall[(pfall['month'] == 2) & (pfall['day'] > 16)].groupby(['year','month','day','hour'])['outcome'].count()\n",
    "#mdcountsall=pfall[(pfall['month'] == 2)].groupby(['year','month','day','hour'])['outcome'].count()\n",
    "#mdcountsall=pfall.groupby(['year','month','day'])['outcome'].count()\n",
    "\n",
    "def createData(pfall,month,outcome) :\n",
    "    if outcome < 2 :\n",
    "        mdcountsall = pfall[(pfall['month'] == month) & (pfall['outcome'] == outcome)].groupby(['year','month','day','hour'])['outcome'].count()\n",
    "    else :\n",
    "        if (month > 0) & (month < 13) :\n",
    "            mdcountsall = pfall[(pfall['month'] == month)].groupby(['year','month','day','hour'])['outcome'].count()\n",
    "        else :\n",
    "            mdcountsall = pfall.groupby(['year','month','day','hour'])['outcome'].count()    \n",
    "    a,b,c,d = get_ymdh(mdcountsall)\n",
    "    data2 = pd.DataFrame()\n",
    "    data2['date'] = get_ymdh_string(a,b,c,d)\n",
    "    data2['outcome'] =  mdcountsall.reset_index()['outcome'].astype(int) \n",
    "\n",
    "    #for pivot table\n",
    "    data2['hours'] =  d.astype(int) \n",
    "    data2['days']  =  c.astype(int) \n",
    "    return data2\n",
    "\n",
    "def createData_ym(pfall,month,outcome) :\n",
    "    if outcome < 2 :\n",
    "        mdcountsall = pfall[(pfall['outcome'] == outcome)].groupby(['year','month'])['outcome'].count()\n",
    "    else :\n",
    "        if (month > 0) & (month < 13) :\n",
    "            mdcountsall = pfall[(pfall['month'] == month)].groupby(['year','month'])['outcome'].count()\n",
    "        else :\n",
    "            mdcountsall = pfall.groupby(['year','month'])['outcome'].count()    \n",
    "    a,b = get_ym(mdcountsall)\n",
    "    data2 = pd.DataFrame()\n",
    "    data2['date'] = get_ym_string(a,b)\n",
    "    data2['outcome'] =  mdcountsall.reset_index()['outcome'].astype(int) \n",
    "\n",
    "    #for pivot table\n",
    "    #data2['hours'] =  d.astype(int) \n",
    "    #data2['days']  =  c.astype(int) \n",
    "    return data2\n",
    "\n",
    "## heatmap\n",
    "def createHeatmap(piv,title=\"\") :\n",
    "    plt.figure(figsize=(24,8))\n",
    "    plt.title(title)\n",
    "    ax = sns.heatmap(piv, square=True)\n",
    "    plt.setp( ax.xaxis.get_majorticklabels(), rotation=0 )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return ax\n",
    "\n",
    "def createBarplot(md,fx,fy,fontscale,title=\"\") :\n",
    "    sns.set(style='whitegrid', palette='muted', font_scale=fontscale)\n",
    "    plt.figure(figsize=(fx,fy))\n",
    "    plt.title(title)\n",
    "    ax = sns.barplot(x=md['date'], y=md['outcome'], data=md)\n",
    "    plt.setp( ax.xaxis.get_majorticklabels(), rotation=0 )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return ax\n",
    "\n",
    "# kernel density estimate (KDE) \n",
    "def createKDE(data2,fx,fy,fontscale,title=\"\") :\n",
    "    sns.set(style='whitegrid', palette='muted', font_scale=fontscale)\n",
    "    plt.figure(figsize=(fx,fy))\n",
    "    plt.title(title)\n",
    "    # Histogram \n",
    "    #ax = sns.distplot(a=data2['outcome'], kde=False)\n",
    "    ax = sns.kdeplot(data=data2['outcome'], shade=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLineplot(md,fx,fy,fontscale,title=\"\",skip=0,rot=90) :\n",
    "    sns.set(style='whitegrid', palette='muted', font_scale=fontscale)\n",
    "    plt.figure(figsize=(fx,fy))\n",
    "    plt.title(title)\n",
    "    ax = sns.lineplot(x=md['date'], y=md['value'], data=md)\n",
    "    for ind, label in enumerate(ax.get_xticklabels()):\n",
    "        if ind % skip == 0:  # every 10th label is kept\n",
    "            label.set_visible(True)\n",
    "            label.set_rotation(rot)\n",
    "        else:\n",
    "            label.set_visible(False)\n",
    "    #plt.setp( ax.xaxis.get_majorticklabels(), rotation=90 )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return ax\n",
    "\n",
    "#ax = createLineplot(pfall,16,10,1.4,title=\"\")\n",
    "#label(ax,500,80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series Anomaly Detection with LSTM Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 22, 10\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "OUTCOME = 'close'\n",
    "\n",
    "TIME_STEPS = 24\n",
    "#TIME_STEPS = 30\n",
    "#TIME_STEPS = 720\n",
    "#TIME_STEPS = 168\n",
    "#TIME_STEPS = 336\n",
    "\n",
    "# setup data (current)\n",
    "def createDataframe(pfall) :\n",
    "    data3 = createData(pfall,0,2)\n",
    "    df = pd.DataFrame()\n",
    "    df[OUTCOME] = data3['outcome']\n",
    "    df.set_index(data3['date'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def getTrainAndTest(df,TRAIN_SIZE) :\n",
    "    train_size = int(len(df) * TRAIN_SIZE)\n",
    "    test_size = len(df) - train_size\n",
    "    train, test = df.iloc[0:train_size], df.iloc[train_size:len(df)]\n",
    "    print(\"train.shape: \",train.shape, \"test.shape: \", test.shape)\n",
    "    return train, test\n",
    "\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)        \n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def initmodel():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.LSTM(\n",
    "        units=64, \n",
    "        input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "    ))\n",
    "    model.add(keras.layers.Dropout(rate=0.2))\n",
    "    model.add(keras.layers.RepeatVector(n=X_train.shape[1]))\n",
    "    model.add(keras.layers.LSTM(units=64, return_sequences=True))\n",
    "    model.add(keras.layers.Dropout(rate=0.2))\n",
    "    model.add(keras.layers.TimeDistributed(keras.layers.Dense(units=X_train.shape[2])))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def testScoreDF(model, THRESHOLD) : \n",
    "    X_test_pred = model.predict(X_test)\n",
    "    test_mae_loss = np.mean(np.abs(X_test_pred - X_test), axis=1)\n",
    "\n",
    "    test_score_df = pd.DataFrame(index=test[TIME_STEPS:].index)\n",
    "    test_score_df['loss'] = test_mae_loss\n",
    "    test_score_df['threshold'] = THRESHOLD\n",
    "    test_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold\n",
    "    test_score_df[OUTCOME] = test[TIME_STEPS:][OUTCOME]\n",
    "    return test_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingSchemaids = [ 1480883705,  -404024316,  -183769575,  2031641327, -1576843338,\n",
    "                     -660710506,  -208259125,  -650561809,   -39609584, -1115728007]\n",
    "\n",
    "def withoutMissingSchemas(pf):\n",
    "    return pf[~pf['schemaid'].isin(missingSchemaids)]\n",
    "\n",
    "def getHashValues(pf):\n",
    "    return pd.unique(pf['hashvalue'])\n",
    "\n",
    "# max timestamp for splits\n",
    "def getMaxTimestamp(pf):\n",
    "    return pf.loc[pf['timestamp'].idxmax()]['timestamp']\n",
    "\n",
    "def getCountDF(pf,column,hashes):\n",
    "    dft = pd.DataFrame(columns=[column, 'count'])\n",
    "    i=0\n",
    "    for hash in hashes:\n",
    "        pfall=pf[pf[column] == hash]\n",
    "        num=len(pd.unique(pfall['value']))\n",
    "        dft.loc[i] = [hash] + [num]\n",
    "        i=i+1\n",
    "    return dft.sort_values('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "#sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "\n",
    "#sc = SparkContext()\n",
    "#sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "sparkSession = SparkSession.builder.config('spark.local.dir', '/tmp').config(\"spark.executor.memory\", \"8g\").config(\"spark.driver.memory\", \"8g\").config(\"spark.driver.maxResultSize\", \"0\").appName(\"example-pyspark-read-and-write\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sparkSession.read.parquet('hdfs://172.30.17.145:8020/user/admin/stage1/flat_7/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show()\n",
    "pf = df.limit(1000000).dropDuplicates().orderBy('timestamp').toPandas()\n",
    "getMaxTimestamp(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1961237651 , 632085806\n",
    "#pf[(pf['hashvalue'] == 1961237651) & (pf['timestamp'] == 1583856953)] \n",
    "pf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = pf[(pf['hashvalue'] == 632085806) & (pf['timestamp'] == 1583856953)] \n",
    "tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMissingSchemaids(pf):\n",
    "    a = np.array([])\n",
    "    for hash in hashes:  \n",
    "        if len(pf[pf['hashvalue'] == hash]) != len(pd.unique(pf[pf['hashvalue'] == hash]['timestamp'])):\n",
    "            value = pd.unique(pf[pf['hashvalue'] == hash]['schemaid'])\n",
    "            if (value in a) == False:\n",
    "                a = np.append(a,value)\n",
    "    return a.astype(int)\n",
    "\n",
    "def usedcolumns(tb):\n",
    "    col = []\n",
    "    for column in tb.columns:\n",
    "        if tb.iloc[0][column] != None :\n",
    "            col.append(column)\n",
    "    return col\n",
    "\n",
    "def keepcolumns(tb,keep):\n",
    "    for column in tb.columns:\n",
    "        if column not in keep :\n",
    "            del(tb[column])\n",
    "\n",
    "            \n",
    "            \n",
    "#used = usedcolumns(tb)\n",
    "#testdel(tb,used)\n",
    "#tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pf.head()\n",
    "#pf[pf['hashvalue'] == 1636706336].head()\n",
    "#,pf[pf['hashvalue'] == -1407530683].iloc[1]\n",
    "len(pd.unique(pf['hashvalue']))\n",
    "hashes = pd.unique(pf['hashvalue'])\n",
    "\n",
    "#len(pf[pf['hashvalue'] == hashes[5]]),len(pd.unique(pf[pf['hashvalue'] == hashes[5]]['timestamp']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf['schemaid'] = pf['schemaid'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "  \n",
    "#missingSchemaids = getMissingSchemaids(pf)  \n",
    "#missingSchemaids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = withoutMissingSchemas(pf)\n",
    "hashes = getHashValues(pf)\n",
    "counts = getCountDF(pf,hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMaxTimestamp(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "cat_features\n",
    "dataall['service'] = dataall['service'].astype(str)\n",
    "dataall['__name__'] = dataall['__name__'].astype(str)\n",
    "dataall['instance'] = dataall['instance'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = encodeall(dataall,cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall=pf[pf['hashvalue'] == dft.loc[13844]['hashvalue']].reset_index()\n",
    "pfall['value'] = pfall['value'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash=360972192\n",
    "len(pf[pf['hashvalue'] == hash]),len(pd.unique(pf[pf['hashvalue'] == hash]['timestamp']))\n",
    "pf[pf['hashvalue'] == hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/50041551/tell-labelenocder-to-ignore-new-labels\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "\n",
    "class TolerantLabelEncoder(LabelEncoder):\n",
    "    def __init__(self, ignore_unknown=False,\n",
    "                       unknown_original_value='unknown', \n",
    "                       unknown_encoded_value=-1):\n",
    "        self.ignore_unknown = ignore_unknown\n",
    "        self.unknown_original_value = unknown_original_value\n",
    "        self.unknown_encoded_value = unknown_encoded_value\n",
    "\n",
    "    def transform(self, y):\n",
    "        check_is_fitted(self, 'classes_')\n",
    "        y = column_or_1d(y, warn=True)\n",
    "\n",
    "        indices = np.isin(y, self.classes_)\n",
    "        if not self.ignore_unknown and not np.all(indices):\n",
    "            raise ValueError(\"y contains new labels: %s\" \n",
    "                                         % str(np.setdiff1d(y, self.classes_)))\n",
    "\n",
    "        y_transformed = np.searchsorted(self.classes_, y)\n",
    "        y_transformed[~indices]=self.unknown_encoded_value\n",
    "        return y_transformed\n",
    "\n",
    "    def inverse_transform(self, y):\n",
    "        check_is_fitted(self, 'classes_')\n",
    "\n",
    "        labels = np.arange(len(self.classes_))\n",
    "        indices = np.isin(y, labels)\n",
    "        if not self.ignore_unknown and not np.all(indices):\n",
    "            raise ValueError(\"y contains new labels: %s\" \n",
    "                                         % str(np.setdiff1d(y, self.classes_)))\n",
    "\n",
    "        y_transformed = np.asarray(self.classes_[y], dtype=object)\n",
    "        y_transformed[~indices]=self.unknown_original_value\n",
    "        return y_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEncoders(dataall,columns):\n",
    "    for column in columns:\n",
    "        le = TolerantLabelEncoder(ignore_unknown=True)\n",
    "        #le.fit([1, 2, 2, 6])\n",
    "        le.fit(dataall[column])\n",
    "        LabelEncoder()\n",
    "        print(le.classes_)\n",
    "        np.save(column + '.npy', le.classes_)\n",
    "        \n",
    "def encode(dataall,columns):\n",
    "    # save np.load\n",
    "    np_load_old = np.load\n",
    "\n",
    "    # modify the default parameters of np.load\n",
    "    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "    for column in columns:\n",
    "        encoder = TolerantLabelEncoder(ignore_unknown=True)\n",
    "        encoder.classes_ = np.load(column + '.npy')\n",
    "        dataall[column] = encoder.transform(dataall[column]) \n",
    "\n",
    "    # restore np.load for future normal usage\n",
    "    np.load = np_load_old\n",
    "    \n",
    "def getEncoder(column):\n",
    "    # save np.load\n",
    "    np_load_old = np.load\n",
    "\n",
    "    # modify the default parameters of np.load\n",
    "    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "    encoder = TolerantLabelEncoder(ignore_unknown=True)\n",
    "    encoder.classes_ = np.load(column + '.npy')\n",
    "        \n",
    "    # restore np.load for future normal usage\n",
    "    np.load = np_load_old\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split instance name\n",
    "\n",
    "def igroup(x):\n",
    "    if x[0:3] == 'cls':\n",
    "        res = x.split('0')\n",
    "        return res[0]\n",
    "    return x\n",
    "    \n",
    "\n",
    "def inode(x):\n",
    "    if x[0:3] == 'cls':\n",
    "        res = x.split('0')\n",
    "        return res[1].split(':')[0]\n",
    "    return '1'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "le = LabelEncoder()\n",
    "#le.fit([1, 2, 2, 6])\n",
    "le.fit(dataall['adapter'])\n",
    "LabelEncoder()\n",
    "le.classes_\n",
    "#np.save('classes.npy', le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le.transform(dataall['adapter']) \n",
    "#dataall['adapter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = np.load('classes.npy')\n",
    "encoder.transform([1, 1, 2, 6,6,1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "cat_features = ['service', '__name__','instance']\n",
    "\n",
    "def encodeall(pfall,cat_features):\n",
    "    #Prepping categorical variables\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    encoder = LabelEncoder()\n",
    "    # Apply the label encoder to each column\n",
    "    encodedpfall = pfall[cat_features].apply(encoder.fit_transform)\n",
    "    return encodedpfall\n",
    "  \n",
    "def getpfbasic(pfall) :   \n",
    "    #pfall = df2.limit(5000000).toPandas()    \n",
    "    #print(pfall.loc[pfall['CSTARTTIME'].idxmax()]['CSTARTTIME'],len(pfall.index))\n",
    "    pfall = pfall.assign(outcome=(~( ((pfall['CSTATUS'] == 'PENDING') & (pfall['CSERVICE'] == 'InvoicePortal')) | ((pfall['CSTATUS'] == 'PENDING') & (pfall['CSERVICE'] == 'IDS')) | (pfall['CSTATUS'] == 'SUCCESS') | (pfall['CSTATUS'] == 'SUCCESS_DOWNLOADED') | (pfall['CSTATUS'] == 'SUCCESS_POLLQUEUE'))).astype(int))\n",
    "    converttimestampcolumnn(pfall,'CSTARTTIME')\n",
    "    pfall['CGLOBALMESSAGEID'] = pfall['CGLOBALMESSAGEID'].apply(hash)\n",
    "    #pfall['CSENDERENDPOINTID'] = pfall['CSENDERENDPOINTID'].astype(str)   \n",
    "    #pfall['CRECEIVERENDPOINTID'] = pfall['CRECEIVERENDPOINTID'].astype(str)\n",
    "    pfall['CMESSAGETAT2'] = pfall['CMESSAGETAT2'].astype(int)\n",
    "    pfall['CSLATAT'] = pfall['CSLATAT'].astype(int)\n",
    "    pfall['CINBOUNDSIZE'] = pfall['CINBOUNDSIZE'].astype(int)\n",
    "    return pfall\n",
    "\n",
    "def getpf(df2) :    \n",
    "    pfall = getpfbasic(df2)\n",
    "    encodedpfall = encodeall(pfall,cat_features)\n",
    "    dataall = pfall[['CGLOBALMESSAGEID','CMESSAGETAT2','CSLATAT','CINBOUNDSIZE', 'outcome']].join(encodedpfall)\n",
    "    adddatecolumns(dataall,pfall)    \n",
    "    return dataall\n",
    "\n",
    "def astype(pfall,selected,newtype):\n",
    "    for each in selected:\n",
    "        pfall[each] = pfall[each].astype(newtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "\n",
    "selected = ['date', 'timestamp', 'value', 'product', 'service', '__name__','instance',  'schemaid', 'hashvalue']\n",
    "#selected = ['date','timestamp','value','product','service','__name__','instance','job','le','port','schemaid','hashvalue']\n",
    "#selected = ['timestamp', 'value', 'schemaid', 'hashvalue']\n",
    "\n",
    "def getpfall(df,selected) :\n",
    "    #pfall = df.limit(5000000).toPandas()  \n",
    "    pfall = df.toPandas() \n",
    "    pfall['timestamp'] = pfall['timestamp'].astype(int)\n",
    "    pfall['schemaid'] = pfall['schemaid'].astype(int)\n",
    "    pfall['hashvalue'] = pfall['hashvalue'].astype(int)\n",
    "    #pfall['value'] = pfall['value'].astype(float)\n",
    "    #for each in selected:\n",
    "    #    pfall[each] = pfall[each].astype(str)\n",
    "    if len(pfall) == 0:\n",
    "        return pfall,0,0\n",
    "    #used = usedcolumns(pfall)\n",
    "    #keepcolumns(pfall,used)\n",
    "    return pfall, int(pfall.loc[pfall['timestamp'].astype(int).idxmax()]['timestamp']),len(pfall.index)        \n",
    "\n",
    "def gettest(to,selected) :\n",
    "    df2 = df.withColumn('timestamp', col('timestamp').cast('long')).filter(col(\"timestamp\") < to ).select(selected).dropDuplicates().orderBy('timestamp')    \n",
    "    return getpfall(df2,selected)\n",
    "\n",
    "def getdata_lt(to,selected=[], filter_names = \"\") :\n",
    "    if len(filter_names) > 0:\n",
    "        print (len(filter_names))\n",
    "        df2 = df.filter(col(\"__name__\") == filter_names ).withColumn('timestamp', col('timestamp').cast('long')).filter(col(\"timestamp\") < to ).dropDuplicates().orderBy('timestamp') \n",
    "    else:\n",
    "        df2 = df.withColumn('timestamp', col('timestamp').cast('long')).filter(col(\"timestamp\") < to ).dropDuplicates().orderBy('timestamp')  \n",
    "    #df2 = df.filter(col(\"__name__\") == 'bis_adapter_finished_processes_total' ).withColumn('timestamp', col('timestamp').cast('long')).filter(col(\"timestamp\") < to ).select(selected).dropDuplicates().orderBy('timestamp') \n",
    "    #dataall = getpf(df2)    \n",
    "    #return dataall\n",
    "    return getpfall(df2,selected)\n",
    "    #return df2\n",
    "    \n",
    "\n",
    "# >=from  <=to    \n",
    "def getdata_ft(_from,_diff,selected=[], filter_names = \"\") :\n",
    "    to = _from + _diff\n",
    "    if len(filter_names) == 0:\n",
    "        print(_from,to)\n",
    "        df2 = df.withColumn('timestamp', col('timestamp').cast('long')) \\\n",
    "                .filter(col(\"timestamp\") >= _from ) \\\n",
    "                .filter(col(\"timestamp\") <= to ) \\\n",
    "                .filter(~df['schemaid'].isin(*missingSchemaids)) \n",
    "                #.orderBy('timestamp') \n",
    "        print((df2.count(), len(df2.columns)))\n",
    "    else:    \n",
    "        df2 = df.filter(col(\"__name__\") == filter_names ).withColumn('timestamp', col('timestamp').cast('long')).filter(col(\"timestamp\") >= _from ).filter(col(\"timestamp\") <= to ).dropDuplicates().orderBy('timestamp') \n",
    "    return getpfall(df2,selected)\n",
    "\n",
    "# >from  <=to   \n",
    "def getdata_gt(_from,_diff,selected=[], filter_names = \"\") :\n",
    "    to = _from + _diff\n",
    "    if len(filter_names) == 0:\n",
    "        print(_from,to)\n",
    "        df2 = df.withColumn('timestamp', col('timestamp').cast('long')) \\\n",
    "                .filter(col(\"timestamp\") > _from ) \\\n",
    "                .filter(col(\"timestamp\") <= to ) \\\n",
    "                .filter(~df['schemaid'].isin(*missingSchemaids)) \n",
    "                #.orderBy('timestamp') \n",
    "        print((df2.count(), len(df2.columns)))\n",
    "    else:    \n",
    "        df2 = df.filter(col(\"__name__\") == filter_names ).withColumn('timestamp', col('timestamp').cast('long')).filter(col(\"timestamp\") >= _from ).filter(col(\"timestamp\") <= to ).dropDuplicates().orderBy('timestamp') \n",
    "    return getpfall(df2,selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected = ['__name__']\n",
    "#df4 = df.select(selected).dropDuplicates().toPandas() \n",
    "#len(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missingSchemaids\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row1 = df.agg({\"timestamp\": \"min\"}).collect()[0]\n",
    "row2 = df.agg({\"timestamp\": \"max\"}).collect()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (row1)\n",
    "dfminTimestamp = row1[\"min(timestamp)\"]\n",
    "print (row2)\n",
    "dfmaxTimestamp = row2[\"max(timestamp)\"]\n",
    "\n",
    "dfrowCount    = df.count()\n",
    "dfcolumnCount = len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_from =  dfminTimestamp\n",
    "to = _from + 100\n",
    "timestamp_diff = 3000\n",
    "\n",
    "df2 = getdata_ft(_from,timestamp_diff,selected=[], filter_names = \"\")\n",
    "\n",
    "#df2.show()\n",
    "#print((df2.count(), len(df2.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "withoutColumns = ['date', 'timestamp', 'value','instance','schemaid', 'hashvalue']\n",
    "columns = df.limit(1).toPandas().columns\n",
    "columns = columns[~columns.isin(withoutColumns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import time\n",
    "import datetime as dt\n",
    "import calendar\n",
    "import pytz\n",
    "de = pytz.timezone('Europe/Berlin')\n",
    "\n",
    "def date(x):\n",
    "    return  dt.datetime.fromtimestamp(float(x), tz=de)\n",
    "\n",
    "\n",
    "def adddatecolumns(data,pf,column) :\n",
    "    data['year'] = pf[column].apply(lambda x: date(x).date().year)\n",
    "    data['month'] = pf[column].apply(lambda x: date(x).date().month)\n",
    "    data['day'] = pf[column].apply(lambda x: date(x).date().day)\n",
    "    data['hour'] = pf[column].apply(lambda x: date(x).time().hour)\n",
    "    data['minute'] = pf[column].apply(lambda x: date(x).time().minute)\n",
    "    #data['second'] = pf[column].apply(lambda x: x.time().second)\n",
    "    #data['microsecond'] = pf[column].apply(lambda x: x.time().microsecond)\n",
    "\n",
    "def converttimestampcolumnn(pf,tsc) :\n",
    "    pf[tsc] = pf[tsc].apply(lambda x: dt.datetime.fromtimestamp(float(x) / 1e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeDataframe(dataall):\n",
    "    astype(dataall,columns,str)\n",
    "    encode(dataall,columns)\n",
    "\n",
    "    del dataall['date']\n",
    "    #del dataall['timestamp']\n",
    "    \n",
    "    astype(dataall,['instance'],str)\n",
    "    dataall['igroup'] = dataall['instance'].apply(lambda x: igroup(x))\n",
    "    dataall['inode']  = dataall['instance'].apply(lambda x: inode(x))\n",
    "\n",
    "    #createEncoders(dataall,['igroup'])\n",
    "    encode(dataall,['igroup'])\n",
    "    astype(dataall,['inode'],int)\n",
    "    del dataall['instance']\n",
    "\n",
    "    # convert timestamp to datetime and add column date\n",
    "    import calendar\n",
    "    import pytz\n",
    "    de = pytz.timezone('Europe/Berlin')\n",
    "    #dataall['date'] = dataall['timestamp'].apply(lambda x: dt.datetime.fromtimestamp(float(x), tz=de))\n",
    "    adddatecolumns(dataall,dataall,'timestamp')\n",
    "    #del dataall['date'] \n",
    "    return dataall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = df.select('timestamp').toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest['date'] = dftest['timestamp'].apply(lambda x: dt.datetime.fromtimestamp(float(x), tz=de))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adddatecolumns(dftest,dftest,'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[0].head()\n",
    "import calendar\n",
    "import pytz\n",
    "de = pytz.timezone('Europe/Berlin')\n",
    "dt.datetime.fromtimestamp(float(df2[0].iloc[0]['timestamp']), tz=de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataall = encodeDataframe(df2[0])\n",
    "timestamp = df2[1]\n",
    "timestamp_diff = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataall = withoutMissingSchemas(dataall)\n",
    "#dataall.head()\n",
    "\n",
    "dataall.to_parquet('/tmp/myfile_7_' + str(timestamp) + '.parquet', engine='fastparquet', compression='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm /tmp/*.parquet\n",
    "#!ls -lht /tmp/*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max timestamp\n",
    "dataall.loc[dataall['timestamp'].idxmax()]['timestamp'], timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk(timestamp,max_cycles=1,timestamp_diff=1000):\n",
    "    cycle = 0\n",
    "    while True:\n",
    "        #next\n",
    "        df2 = getdata_gt(timestamp,timestamp_diff)\n",
    "        encodeDataframe(df2[0])\n",
    "\n",
    "        #merge and prepare for next step\n",
    "        #dataall = dataall.append(df2[0], ignore_index=True)\n",
    "        timestamp = df2[1]\n",
    "        print(timestamp,df2[2])\n",
    "        df2[0].to_parquet('/tmp/myfile_7_' + str(timestamp) + '.parquet', engine='fastparquet', compression='GZIP')\n",
    "        \n",
    "        if df2[2] == 0:\n",
    "            break  \n",
    "        cycle = cycle + 1\n",
    "        if cycle == max_cycles:\n",
    "            break\n",
    "    return timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1584187648 1584190648\n",
      "(2715976, 58)\n",
      "1584190631 2715976\n",
      "1584190631 1584193631\n",
      "(2741692, 58)\n",
      "1584193621 2741692\n",
      "1584193621 1584196621\n",
      "(2741636, 58)\n",
      "1584196610 2741636\n",
      "1584196610 1584199610\n",
      "(2715982, 58)\n",
      "1584199591 2715982\n",
      "1584199591 1584202591\n",
      "(2741722, 58)\n",
      "1584202581 2741722\n",
      "1584202581 1584205581\n",
      "(2741692, 58)\n",
      "1584205569 2741692\n",
      "1584205569 1584208569\n",
      "(2741654, 58)\n",
      "1584208559 2741654\n",
      "1584208559 1584211559\n",
      "(2715974, 58)\n",
      "1584211539 2715974\n",
      "1584211539 1584214539\n",
      "(2741718, 58)\n",
      "1584214531 2741718\n",
      "1584214531 1584217531\n",
      "(2741618, 58)\n",
      "1584217523 2741618\n",
      "1584217523 1584220523\n",
      "(2715930, 58)\n",
      "1584220505 2715930\n",
      "1584220505 1584223505\n",
      "(2741876, 58)\n",
      "1584223494 2741876\n",
      "1584223494 1584226494\n",
      "(2741606, 58)\n",
      "1584226485 2741606\n",
      "1584226485 1584229485\n",
      "(2715461, 58)\n",
      "1584229466 2715461\n",
      "1584229466 1584232466\n",
      "(2741902, 58)\n",
      "1584232459 2741902\n",
      "1584232459 1584235459\n",
      "(2742026, 58)\n",
      "1584235450 2742026\n",
      "1584235450 1584238450\n",
      "(2741890, 58)\n",
      "1584238441 2741890\n",
      "1584238441 1584241441\n",
      "(2716340, 58)\n",
      "1584241424 2716340\n",
      "1584241424 1584244424\n",
      "(2741836, 58)\n",
      "1584244413 2741836\n",
      "1584244413 1584247413\n",
      "(2741938, 58)\n",
      "1584247402 2741938\n",
      "1584247402 1584250402\n",
      "(2716402, 58)\n",
      "1584250384 2716402\n",
      "1584250384 1584253384\n",
      "(2741964, 58)\n",
      "1584253375 2741964\n",
      "1584253375 1584256375\n",
      "(2742026, 58)\n",
      "1584256364 2742026\n",
      "1584256364 1584259364\n",
      "(2716552, 58)\n",
      "1584259346 2716552\n",
      "1584259346 1584262346\n",
      "(2744685, 58)\n",
      "1584262337 2744685\n",
      "1584262337 1584265337\n",
      "(2745399, 58)\n",
      "1584265325 2745399\n",
      "1584265325 1584268325\n",
      "(2745455, 58)\n",
      "1584268313 2745455\n",
      "1584268313 1584271313\n",
      "(2719640, 58)\n",
      "1584271294 2719640\n",
      "1584271294 1584274294\n",
      "(2745225, 58)\n",
      "1584274285 2745225\n",
      "1584274285 1584277285\n",
      "(2745195, 58)\n",
      "1584277275 2745195\n",
      "1584277275 1584280275\n",
      "(2719622, 58)\n",
      "1584280255 2719622\n",
      "1584280255 1584283255\n",
      "(2745239, 58)\n",
      "1584283246 2745239\n",
      "1584283246 1584286246\n",
      "(2745527, 58)\n",
      "1584286232 2745527\n",
      "1584286232 1584289232\n",
      "(2718964, 58)\n",
      "1584289215 2718964\n",
      "1584289215 1584292215\n",
      "(2745197, 58)\n",
      "1584292203 2745197\n",
      "1584292203 1584295203\n",
      "(2745227, 58)\n",
      "1584295193 2745227\n",
      "1584295193 1584298193\n",
      "(2745095, 58)\n",
      "1584298184 2745095\n",
      "1584298184 1584301184\n",
      "(2719692, 58)\n",
      "1584301167 2719692\n",
      "1584301167 1584304167\n",
      "(2745109, 58)\n",
      "1584304155 2745109\n",
      "1584304155 1584307155\n",
      "(2745095, 58)\n",
      "1584307146 2745095\n",
      "1584307146 1584310146\n",
      "(2719516, 58)\n",
      "1584310129 2719516\n",
      "1584310129 1584313129\n",
      "(2745041, 58)\n",
      "1584313120 2745041\n",
      "1584313120 1584316120\n",
      "(2745532, 58)\n",
      "1584316113 2745532\n",
      "1584316113 1584319113\n",
      "(2719820, 58)\n",
      "1584319097 2719820\n",
      "1584319097 1584322097\n",
      "(2745401, 58)\n",
      "1584322087 2745401\n",
      "1584322087 1584325087\n",
      "(2745433, 58)\n",
      "1584325077 2745433\n",
      "1584325077 1584328077\n",
      "(2745481, 58)\n",
      "1584328068 2745481\n",
      "1584328068 1584331068\n",
      "(2719716, 58)\n",
      "1584331052 2719716\n",
      "1584331052 1584334052\n",
      "(2745463, 58)\n",
      "1584334045 2745463\n",
      "1584334045 1584337045\n",
      "(2745543, 58)\n",
      "1584337037 2745543\n",
      "1584337037 1584340037\n",
      "(2719916, 58)\n",
      "1584340019 2719916\n",
      "1584340019 1584343019\n",
      "(2745701, 58)\n",
      "1584343009 2745701\n",
      "1584343009 1584346009\n",
      "(2745685, 58)\n",
      "1584346002 2745685\n",
      "1584346002 1584349002\n",
      "(2720602, 58)\n",
      "1584348987 2720602\n",
      "1584348987 1584351987\n",
      "(2746077, 58)\n",
      "1584351980 2746077\n",
      "1584351980 1584354980\n",
      "(2711211, 58)\n",
      "1584354972 2711211\n",
      "1584354972 1584357972\n",
      "(2583298, 58)\n",
      "1584357960 2583298\n",
      "1584357960 1584360960\n",
      "(2572963, 58)\n",
      "1584360941 2572963\n",
      "1584360941 1584363941\n",
      "(2677243, 58)\n",
      "1584363932 2677243\n",
      "1584363932 1584366932\n",
      "(2665174, 58)\n",
      "1584366926 2665174\n",
      "1584366926 1584369926\n",
      "(2676158, 58)\n",
      "1584369910 2676158\n",
      "1584369910 1584372910\n",
      "(2701709, 58)\n",
      "1584372902 2701709\n",
      "1584372902 1584375902\n",
      "(2702070, 58)\n",
      "1584375891 2702070\n",
      "1584375891 1584378891\n",
      "(2676636, 58)\n",
      "1584378874 2676636\n",
      "1584378874 1584381874\n",
      "(2701902, 58)\n",
      "1584381861 2701902\n",
      "1584381861 1584384861\n",
      "(2702044, 58)\n",
      "1584384850 2702044\n",
      "1584384850 1584387850\n",
      "(2702267, 58)\n",
      "1584387841 2702267\n",
      "1584387841 1584390841\n",
      "(2676916, 58)\n",
      "1584390822 2676916\n",
      "1584390822 1584393822\n",
      "(2702269, 58)\n",
      "1584393811 2702269\n",
      "1584393811 1584396811\n",
      "(2702679, 58)\n",
      "1584396799 2702679\n",
      "1584396799 1584399799\n",
      "(2678009, 58)\n",
      "1584399780 2678009\n",
      "1584399780 1584402780\n",
      "(2713198, 58)\n",
      "1584402768 2713198\n",
      "1584402768 1584405768\n",
      "(2713174, 58)\n",
      "1584405757 2713174\n",
      "1584405757 1584408757\n",
      "(2687692, 58)\n",
      "1584408739 2687692\n",
      "1584408739 1584411739\n",
      "(2713098, 58)\n",
      "1584411727 2713098\n",
      "1584411727 1584414727\n",
      "(2713050, 58)\n",
      "1584414716 2713050\n",
      "1584414716 1584417716\n",
      "(2713062, 58)\n",
      "1584417706 2713062\n",
      "1584417706 1584420706\n",
      "(2687726, 58)\n",
      "1584420687 2687726\n",
      "1584420687 1584423687\n",
      "(2713312, 58)\n",
      "1584423677 2713312\n",
      "1584423677 1584426677\n",
      "(2713722, 58)\n",
      "1584426666 2713722\n",
      "1584426666 1584429666\n",
      "(2689847, 58)\n",
      "1584429646 2689847\n",
      "1584429646 1584432646\n",
      "(2547785, 58)\n",
      "1584432635 2547785\n",
      "1584432635 1584435635\n",
      "(2475568, 58)\n",
      "1584435614 2475568\n",
      "1584435614 1584438614\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-0042080da6ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1584187648\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-3869ff523163>\u001b[0m in \u001b[0;36mwalk\u001b[0;34m(timestamp, max_cycles, timestamp_diff)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m#next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetdata_gt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimestamp_diff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mencodeDataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-5b82bd506294>\u001b[0m in \u001b[0;36mgetdata_gt\u001b[0;34m(_from, _diff, selected, filter_names)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'schemaid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmissingSchemaids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;31m#.orderBy('timestamp')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__name__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfilter_names\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'long'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0m_from\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropDuplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \"\"\"\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "timestamp = 1584187648\n",
    "timestamp = walk(timestamp,1000,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.unique(dataall['hashvalue']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = getdata_ft(timestamp,timestamp_diff,selected=[], filter_names = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(df3[0]['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_lt = 1583856983\n",
    "#start_lt = 1585498272\n",
    "\n",
    "timestamp_diff = 5000\n",
    "dataall2 = getdata_lt(start_lt,selected)\n",
    "dataall = dataall2[0]\n",
    "timestamp = dataall2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestamp_diff = 10\n",
    "def walk(dataall,timestamp,max_cycles=1):\n",
    "    cycle = 0\n",
    "    while True:\n",
    "        dataall2 = getdata_gt(timestamp ,timestamp_diff,selected)\n",
    "        print(dataall2[2])\n",
    "        if dataall2[2] == 0:\n",
    "            break  \n",
    "        dataall = dataall.append(dataall2[0], ignore_index=True)\n",
    "        cycle = cycle + 1\n",
    "        if cycle == max_cycles:\n",
    "            break\n",
    "        timestamp = dataall2[1]\n",
    "    return dataall,dataall2[1],dataall2[2]\n",
    "\n",
    "#timestamp = dataall2[1]\n",
    "dataall,timestamp,length = walk(dataall,timestamp,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_diff = 10\n",
    "def walk(dataall2,dataall,max_cycles=1):\n",
    "    cycle = 0\n",
    "    while True:\n",
    "        dataall2 = getdata_gt(dataall2[1] ,timestamp_diff,selected)\n",
    "        print(dataall2[2])\n",
    "        if dataall2[2] == 0:\n",
    "            break  \n",
    "        dataall = dataall.append(dataall2[0], ignore_index=True)\n",
    "        cycle = cycle + 1\n",
    "        if cycle == max_cycles:\n",
    "            break\n",
    "    return dataall,dataall2[1],dataall2[2]\n",
    "\n",
    "dataall,timestamp,length = walk(dataall2,dataall,1)\n",
    "dataall = dataall2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testEncoding\n",
    "#pd.unique(dataall['__name__'])\n",
    "#astype(dataall,['adapter'],str)\n",
    "#pd.unique(dataall['__name__'])\n",
    "dataall.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataall to dataframe and store it to hdfs\n",
    "#dataall\n",
    "#dfm = sparkSession.createDataFrame(dataall)\n",
    "#dfm.write.parquet('hdfs://172.30.17.145:8020/user/admin/bis_adapter_finished_processes_total_7.parquet')\n",
    "\n",
    "import os\n",
    "#os.environ['http_proxy'] = \"http://172.30.12.56:3128\" \n",
    "#os.environ['https_proxy'] = \"https://172.30.12.56:3128\"    \n",
    "#!conda install -y -c conda-forge fastparquet\n",
    "dataall.to_parquet('/tmp/myfile_7.parquet', engine='fastparquet', compression='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!ls -lh /tmp/myfile*.parquet\n",
    "\n",
    "#pd.read_parquet('/tmp/myfile.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pft = dataall2.toPandas() \n",
    "#dataall\n",
    "#getMaxTimestamp(dataall)\n",
    "#dataall = withoutMissingSchemas(dataall)\n",
    "\n",
    "used = usedcolumns(dataall)\n",
    "#keepcolumns(dataall,used)\n",
    "used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVariableUniqueColums(dataall):\n",
    "    col = []\n",
    "    for column in dataall.columns:\n",
    "        size = len(pd.unique(dataall[column]))\n",
    "        #print(column,size)\n",
    "        if size > 1:\n",
    "            col.append(column)\n",
    "    return col\n",
    "\n",
    "keep = getVariableUniqueColums(dataall)\n",
    "keepcolumns(dataall,keep)\n",
    "keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataall.head()\n",
    "#pd.unique(dataall['adapter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used_columns = ['__name__','adapter','job','logical_system','instance','product','service']\n",
    "used_columns = ['__name__','job','instance','product','service']\n",
    "astype(dataall,used_columns,str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withoutColumns = ['date', 'timestamp', 'value','instance','schemaid', 'hashvalue']\n",
    "columns = df.limit(1).toPandas().columns\n",
    "columns = columns[~columns.isin(withoutColumns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns:\n",
    "    print(column)\n",
    "    df4 = df.select(column).dropDuplicates().toPandas() \n",
    "    df4[column] = df4[column].astype(str)\n",
    "    createEncoders(df4,[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'instance'\n",
    "df4 = df.select(column).dropDuplicates().toPandas() \n",
    "df4[column] = df4[column].astype(str)\n",
    "#createEncoders(df4,[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#astype(dataall,['instance'],str)\n",
    "df4['igroup'] = df4['instance'].apply(lambda x: igroup(x))\n",
    "df4['inode']  = df4['instance'].apply(lambda x: inode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createEncoders(df4,['igroup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = getEncoder(columns[8])\n",
    "print(encoder.inverse_transform([5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = ['__name__']\n",
    "df4 = df.select(selected).dropDuplicates().toPandas() \n",
    "len(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createEncoders(dataall,used_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode(dataall,used_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'bis_adapter_finished_processes_total'\n",
    "#encoder = getEncoder(dataall,'__name__')\n",
    "#encoder = getEncoder('__name__')\n",
    "encoder = getEncoder(used_columns[0])\n",
    "print(encoder.inverse_transform([413]))\n",
    "#print(encoder.transform([f]))\n",
    "#encoder.classes_\n",
    "#createEncoders(dataall,['service'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = TolerantLabelEncoder(ignore_unknown=True)\n",
    "en.fit(['a','b'])\n",
    "\n",
    "print(en.transform(['a', 'c', 'b']))\n",
    "# Output: [ 0 -1  1]\n",
    "\n",
    "print(en.inverse_transform([-1, 0, 1]))\n",
    "# Output: ['unknown' 'a' 'b']\n",
    "\n",
    "en.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('test' + '.npy', en.classes_)\n",
    "getEncoder('test').classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save encoders / cannot adapt to new labels\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "def createEncoders(dataall,columns):\n",
    "    for column in columns:\n",
    "        le = LabelEncoder()\n",
    "        #le.fit([1, 2, 2, 6])\n",
    "        le.fit(dataall[column])\n",
    "        LabelEncoder()\n",
    "        le.classes_\n",
    "        np.save(column + '.npy', le.classes_)\n",
    "    \n",
    "def encode(dataall,columns):\n",
    "    # save np.load\n",
    "    np_load_old = np.load\n",
    "\n",
    "    # modify the default parameters of np.load\n",
    "    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "    for column in columns:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.classes_ = np.load(column + '.npy')\n",
    "        dataall[column] = encoder.transform(dataall[column]) \n",
    "\n",
    "    # restore np.load for future normal usage\n",
    "    np.load = np_load_old\n",
    "    \n",
    "createEncoders(dataall,['__name__','adapter','job','logical_system','product','service'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "encode(dataall,['__name__','adapter','job','logical_system','product','service'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataall = withoutMissingSchemas(dataall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astype(dataall,['instance'],str)\n",
    "dataall['igroup'] = dataall['instance'].apply(lambda x: igroup(x))\n",
    "dataall['inode']  = dataall['instance'].apply(lambda x: inode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createEncoders(dataall,['igroup'])\n",
    "encode(dataall,['igroup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamp to datetime and add column date\n",
    "import calendar\n",
    "import pytz\n",
    "de = pytz.timezone('Europe/Berlin')\n",
    "dataall['date'] = dataall['timestamp'].apply(lambda x: dt.datetime.fromtimestamp(float(x), tz=de))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adddatecolumns(dataall,dataall,'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features=['group']\n",
    "enc = encodeall(dataall,cat_features)\n",
    "dataall['group'] = enc['group']\n",
    "#dataall['instance'] = enc['instance']\n",
    "astype(dataall,['group','node','value'],int)\n",
    "del(dataall['instance']) \n",
    "dataall['service'] = encodeall(dataall,['service'])['service']\n",
    "dataall['logical_system'] = encodeall(dataall,['logical_system'])['logical_system']\n",
    "dataall['job'] = encodeall(dataall,['job'])['job']\n",
    "dataall['adapter'] = encodeall(dataall,['adapter'])['adapter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataall\n",
    "import calendar\n",
    "import pytz\n",
    "res=\"2020-03-10 16:59:59\"\n",
    "x=1583855999\n",
    "de = pytz.timezone('Europe/Berlin')\n",
    "dt.datetime.fromtimestamp(float(x), tz=de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataall.dtypes\n",
    "#dataall.head()\n",
    "dataall[dataall['inode'] == '9']\n",
    "pd.unique(dataall['igroup'])\n",
    "#astype(dataall,'date',dt.datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataall[(dataall['service'] == 3) & (dataall['job'] == 6) & (dataall['adapter'] == 10) & (dataall['inode'] != 4)]['inode']\n",
    "#pd.unique(dataall[dataall['job'] == 'MAKTEST']['service'])\n",
    "#pd.unique(dataall[dataall['job'] == 'MAKPRO']['node'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdcountsall = dataall[(dataall['service'] == 3) & (dataall['job'] == 6) & (dataall['adapter'] == 10) & (dataall['node'] == 3)]\n",
    "mdcountsall = mdcountsall.groupby(['year','month','day','hour'])['value'].count()    \n",
    "len(mdcountsall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = get_ymdh(mdcountsall)\n",
    "data2 = pd.DataFrame()\n",
    "data2['date'] = get_ymdh_string(a,b,c,d)\n",
    "data2['value'] =  mdcountsall.reset_index()['value'].astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = createLineplot(data2,16,10,1.4,title=\"\",skip=10)\n",
    "label(ax,10,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def label(graph,skip,rot) :\n",
    "    for ind, label in enumerate(graph.get_xticklabels()):\n",
    "        if ind % skip == 0:  # every 10th label is kept\n",
    "            label.set_visible(True)\n",
    "            label.set_rotation(rot)\n",
    "        else:\n",
    "            label.set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataall = withoutMissingSchemas(dataall)\n",
    "hashes = getHashValues(dataall)\n",
    "countpf = getCountDF(dataall,'hashvalue',hashes).reset_index()\n",
    "countpf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dataall[dataall['hashvalue'] == countpf.iloc[224]['hashvalue']]\n",
    "\n",
    "ax = createLineplot(b,16,10,1.4,title=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataall[dataall['hashvalue'] == -869302958]\n",
    "row = dataall[dataall['hashvalue'] == countpf.iloc[224]['hashvalue']].iloc[0]\n",
    "\n",
    "cluster = cluster_instances\n",
    "cluster_instances = pd.unique(cluster['instance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.unique(dataall['__name__'])\n",
    "countnames = getCountDF(dataall,'__name__',names).reset_index()\n",
    "countnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.unique(dataall[dataall['__name__'] == 'bis_adapter_finished_processes_total']['schemaid'])\n",
    "pd.unique(dataall[dataall['schemaid'] == 232746291]['__name__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astype(dataall,cat_features,str)\n",
    "enc = encodeall(dataall,cat_features)\n",
    "dataall['service_enc'] = enc['service']\n",
    "dataall['__name___enc'] = enc['__name__']\n",
    "dataall['instance_enc'] = enc['instance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataall[dataall['hashvalue'] == -869302958]\n",
    "row = dataall[dataall['hashvalue'] == -488244161].iloc[0]\n",
    "row['service_enc'], row['__name___enc'], row['instance_enc'],row['schemaid']\n",
    "cluster = dataall[(dataall['service_enc'] == row['service_enc']) &\n",
    "        (dataall['__name___enc'] == row['__name___enc']) &\n",
    "        (dataall['schemaid'] == row['schemaid'])]\n",
    "cluster_instances = pd.unique(cluster['instance_enc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cluster[cluster['instance_enc'] == cluster_instances[0]]\n",
    "pd.unique(a['hashvalue'])\n",
    "#getMissingSchemaids(cluster)\n",
    "#1961237651 , 632085806\n",
    "#cluster[cluster['hashvalue'] == -48824416]\n",
    "\n",
    "#le   = pd.unique(a['le'])\n",
    "#port = pd.unique(a['port'])\n",
    "#a.columns\n",
    "#pd.unique(a['job'])\n",
    "#a.head()\n",
    "#232746291\n",
    "#missingSchemaids\n",
    "\n",
    "for column in a.columns:\n",
    "    print(column,len(pd.unique(a[column])))\n",
    "    \n",
    "adapters=pd.unique(a['adapter'])\n",
    "hashes=pd.unique(a['hashvalue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.loc[row_indexer,col_indexer] = value \n",
    "b=a[(a['hashvalue'] == hashes[4]) ]\n",
    "#b = a[(a['port'] == port[0]) & (a['le'] == le[2]) ]\n",
    "b['value'] = b['value'].astype(int)\n",
    "#pd.unique(b['hashvalue'])\n",
    "#pd.unique(a['port'] == '8443')\n",
    "#a['port']\n",
    "ax = createLineplot(b,16,10,1.4,title=\"\")\n",
    "label(ax,500,80)\n",
    "#b\n",
    "len(b['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number messages sent by endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = createData_ym(pfall,0,2)\n",
    "createBarplot(md,24,9,3.0,title=\"number messages sent by all endpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = createData_ym(pfall,0,1)\n",
    "createBarplot(md,24,9,3.0,title=\"number messages with errors sent by all endpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details for different CSENDERENDPOINTID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.unique(mdcountsall[1].index.get_level_values(0))\n",
    "TOP=500000\n",
    "result = pfall.groupby(['CSENDERENDPOINTID']).count()\n",
    "data2 = pd.DataFrame()\n",
    "data2['date'] = result.index.get_level_values(0).astype(str)\n",
    "data2['outcome'] =  result['outcome'].astype(int)\n",
    "topsender =  data2[data2['outcome'] > TOP].sort_values('outcome').reset_index()\n",
    "topsender.columns = ['index', 'CSENDERENDPOINTID', 'outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pfall1 = pfall[pfall['CSENDERENDPOINTID'].isin(topsender['date'])]\n",
    "pfall1 = pfall[pfall['CSENDERENDPOINTID']==int(topsender.iloc[7]['CSENDERENDPOINTID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = str(len(topsender)) + \" senders with more than \" + str(TOP) + \" messages (kernel density estimate )\"\n",
    "ax = createKDE(data2,16,8,2,title)\n",
    "label(ax,1,90)\n",
    "#topsender.iloc[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number messages of selected endpoint (Msgs / h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"CSENDERENDPOINTID: \" + str(topsender.iloc[7]['CSENDERENDPOINTID']) + \": number messages so far = \" + str(topsender.iloc[7]['outcome']))\n",
    "        \n",
    "\n",
    "def createHeatmapSeries(pfall, months,category, outcome) :\n",
    "    for month in months:\n",
    "        data2 = createData(pfall,month,outcome)\n",
    "        piv = pd.pivot_table(data2, values=\"outcome\",index=[\"hours\"], columns=[\"days\"], fill_value=0)\n",
    "        #titlestring = \"CSENDERENDPOINTID: \" + str(topsender.iloc[7]['CSENDERENDPOINTID']) + \": \"+ category + \" so far = \" + str(topsender.iloc[7]['outcome']) + \" , month: \" + str(month) \n",
    "        titlestring = \"CSENDERENDPOINTID: \" + str(topsender.iloc[7]['CSENDERENDPOINTID']) + \": \"+ category  + \" month: \" + str(month) \n",
    "        sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "        createHeatmap(piv, titlestring)\n",
    "       \n",
    "createHeatmapSeries(pfall1,[10,11,12,1,2,3,4],'messages', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number messages with errors of selected endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "createHeatmapSeries(pfall1, [11,12,1,2,3,4],'messages with errors', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection with LSTM Autoencoders (selected SENDERPOINTID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 22, 10\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = createDataframe(pfall1)\n",
    "train, test = getTrainAndTest(df,0.95)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(train[[OUTCOME]])\n",
    "train[OUTCOME] = scaler.transform(train[[OUTCOME]])\n",
    "test[OUTCOME] = scaler.transform(test[[OUTCOME]])\n",
    "\n",
    "# reshape to [samples, time_steps, n_features]\n",
    "\n",
    "X_train, y_train = create_dataset(train[[OUTCOME]], train.close, TIME_STEPS)\n",
    "X_test, y_test = create_dataset(test[[OUTCOME]], test.close, TIME_STEPS)\n",
    "print(X_train.shape)\n",
    "\n",
    "model = initmodel()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    shuffle=False)\n",
    "\n",
    "X_train_pred = model.predict(X_train)\n",
    "train_mae_loss = np.mean(np.abs(X_train_pred - X_train), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_mae_loss, bins=50, kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_df = testScoreDF(model, 1.5)\n",
    "anomalies     = test_score_df[test_score_df.anomaly == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_score_df.index, test_score_df.loss, label='loss')\n",
    "plt.plot(test_score_df.index, test_score_df.threshold, label='threshold')\n",
    "plt.xticks(rotation=25)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#anomalies.head()\n",
    "#anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "  test[TIME_STEPS:].index, \n",
    "  scaler.inverse_transform(test[TIME_STEPS:].close), \n",
    "  label='msg count'\n",
    ");\n",
    "\n",
    "ax = sns.scatterplot(\n",
    "  anomalies.index,\n",
    "  scaler.inverse_transform(anomalies.close),\n",
    "  color=sns.color_palette()[3],\n",
    "  s=152,\n",
    "  label='anomaly'\n",
    ")\n",
    "plt.xticks(rotation=25)\n",
    "plt.legend();\n",
    "\n",
    "label(ax,5,80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
