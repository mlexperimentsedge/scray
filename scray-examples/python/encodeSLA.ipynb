{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dfBasics\n",
    "import common\n",
    "import encoder\n",
    "import pfAdapt\n",
    "#import charts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def install():\n",
    "    import os\n",
    "    os.environ['http_proxy'] = \"http://172.30.12.56:3128\" \n",
    "    os.environ['https_proxy'] = \"https://172.30.12.56:3128\"  \n",
    "    !pip install pyarrow\n",
    "    #!pip3 install fastparquet\n",
    "    #!conda uninstall fastparquet\n",
    "    #!conda config --add channels conda-forge\n",
    "    #!conda install -y -c conda-forge fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCountDF(pf,column,hashes):\n",
    "    dft = pd.DataFrame(columns=[column, 'count'])\n",
    "    i=0\n",
    "    for hash in hashes:\n",
    "        pfall=pf[pf[column] == hash]\n",
    "        num=len(pfall)\n",
    "        if num > 1:\n",
    "            dft.loc[i] = [hash] + [num]\n",
    "            i=i+1\n",
    "    return dft\n",
    "    #return dft.sort_values('count')\n",
    "\n",
    "def usedcolumns(tb,row):\n",
    "    col = []\n",
    "    for column in tb.columns:\n",
    "        if tb.iloc[row][column] == None :\n",
    "            col.append(column)\n",
    "    return col\n",
    "\n",
    "def diffcolumns(tb):\n",
    "    col = []\n",
    "    for column in tb.columns:\n",
    "        if tb.iloc[0][column] != tb.iloc[1][column] :\n",
    "            col.append(column)\n",
    "    return col\n",
    "\n",
    "def printtt1():    \n",
    "    col = []\n",
    "    for index, row in tt.iterrows():\n",
    "        tb = pfall[pfall['CGLOBALMESSAGEID'] == row['CGLOBALMESSAGEID']]\n",
    "        for bindex, brow in tb.iterrows():\n",
    "            if pfall.loc[bindex]['CSLADELIVERYTIME'] < 0 :\n",
    "                #print (str(index) + ' ' + str(bindex))\n",
    "                col.append(bindex)\n",
    "    return col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def astype(pfall,selected,newtype):\n",
    "    for each in selected:\n",
    "        pfall[each] = pfall[each].astype(newtype)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = [ 'CSTATUS', 'CSERVICE',\\\n",
    "        'CSENDERPROTOCOL', 'CSENDERENDPOINTID',\\\n",
    "        'CRECEIVERPROTOCOL', 'CRECEIVERENDPOINTID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodepfall(pfall,number):\n",
    "    print('1')\n",
    "    astype(pfall,['CSTARTTIME','CENDTIME','CSLATAT','CMESSAGETAT2','CSLADELIVERYTIME','CINBOUNDSIZE'] ,int) \n",
    "    #del(pfall['CSLABILLINGMONTH'])\n",
    "    pfall['CGLOBALMESSAGEID'] = pfall['CGLOBALMESSAGEID'].apply(hash)\n",
    "    print('1b')\n",
    "    ac = pd.unique(pfall['CGLOBALMESSAGEID'])\n",
    "    print('2')\n",
    "    tt = getCountDF(pfall,'CGLOBALMESSAGEID',ac)\n",
    "    print('3')\n",
    "    col = printtt() \n",
    "    print('4')\n",
    "    pfall = pfall.drop(col)\n",
    "    print('5')\n",
    "    astype(pfall,selected,str) \n",
    "    encoder.encode(pfall,selected)\n",
    "    \n",
    "    print('6')\n",
    "    pfall = pfall.drop_duplicates()\n",
    "    print('7')\n",
    "    pfall.to_parquet('/tmp/sla_' + number + '.parquet', engine='fastparquet', compression='GZIP')\n",
    "    return pfall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['CGLOBALMESSAGEID', 'CSTARTTIME', 'CENDTIME', 'CSTATUS', 'CSERVICE',\\\n",
    "       'CSLABILLINGMONTH', 'CSENDERPROTOCOL', 'CSENDERENDPOINTID',\\\n",
    "       'CINBOUNDSIZE', 'CRECEIVERPROTOCOL', 'CRECEIVERENDPOINTID', 'CSLATAT',\\\n",
    "       'CMESSAGETAT2', 'CSLADELIVERYTIME']\n",
    "# withot 'CSLABILLINGMONTH'\n",
    "def get_columns_2():\n",
    "    columns = ['CGLOBALMESSAGEID', 'CSTARTTIME', 'CENDTIME', 'CSTATUS', 'CSERVICE',\\\n",
    "            'CSENDERPROTOCOL', 'CSENDERENDPOINTID',\\\n",
    "           'CINBOUNDSIZE', 'CRECEIVERPROTOCOL', 'CRECEIVERENDPOINTID', 'CSLATAT',\\\n",
    "           'CMESSAGETAT2', 'CSLADELIVERYTIME']\n",
    "    return columns\n",
    "columns = get_columns_2()\n",
    "#to count messages sent\n",
    "#columns = [ 'CSTARTTIME', 'CSENDERENDPOINTID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession = dfBasics.getSparkSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printtt(pfall,tt):    \n",
    "    col = []\n",
    "    #col2 = []\n",
    "    i = 0\n",
    "    for index in tt.index.get_level_values(0):\n",
    "        tb = pfall[pfall['CGLOBALMESSAGEID'] == index]\n",
    "        findex = []\n",
    "        for bindex, brow in tb.iterrows():   \n",
    "            if pfall.loc[bindex]['CSLADELIVERYTIME'] < 0 :\n",
    "                #print (str(index) + ' ' + str(bindex))\n",
    "                #col.append(bindex)\n",
    "                findex.append(bindex)\n",
    "        if(len(findex) == 1):\n",
    "            col.append(findex[0])   \n",
    "        elif(len(findex) == 2):\n",
    "            if (pfall.iloc[findex[0]]['CENDTIME'] > pfall.iloc[findex[1]]['CENDTIME']):\n",
    "                col.append(findex[0])\n",
    "            else:\n",
    "                col.append(findex[1])\n",
    "        else:\n",
    "            if (tb.iloc[0]['CMESSAGETAT2'] > tb.iloc[1]['CMESSAGETAT2']):\n",
    "                col.append(tb.index[0])\n",
    "            else:\n",
    "                col.append(tb.index[1]) \n",
    "    return col\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# slatimestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filetimestamps = list(sparkSession.read.text('hdfs://172.30.17.145:8020/user/admin/slatimestamps.txt').select('value').toPandas()['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getencodedpfall(line) :\n",
    "    try:\n",
    "        #line = filetimestamps[0]\n",
    "        pfall = sparkSession.read.parquet('hdfs://172.30.17.145:8020/sla_sql_data/' + line +  '/*').select(columns).dropDuplicates().toPandas()\n",
    "        astype(pfall,['CSTARTTIME','CENDTIME','CSLATAT','CMESSAGETAT2','CSLADELIVERYTIME','CINBOUNDSIZE'] ,int) \n",
    "        pfall['CGLOBALMESSAGEID'] = pfall['CGLOBALMESSAGEID'].apply(hash)\n",
    "        mdcountsall = pfall.groupby(['CGLOBALMESSAGEID','CSTATUS'])['CSTARTTIME'].count()\n",
    "        mdcountsall = mdcountsall[mdcountsall > 1]\n",
    "        f = printtt(pfall,mdcountsall)\n",
    "        pfall = pfall.drop(f)\n",
    "        astype(pfall,selected,str) \n",
    "        encoder.encode(pfall,selected)\n",
    "        \n",
    "        # check for duplicates\n",
    "        g=pfall.groupby('CGLOBALMESSAGEID')['CGLOBALMESSAGEID'].value_counts()\n",
    "        _duplicates = list(g.where(g>1).dropna().index.get_level_values(0))\n",
    "\n",
    "        _drop_index = []\n",
    "        for id in _duplicates:\n",
    "            index = pfall[pfall['CGLOBALMESSAGEID'] == id].sort_values('CENDTIME').index\n",
    "            _drop_index.append(index[0])\n",
    "     \n",
    "        # drop duplicates    \n",
    "        pfall = pfall.drop(_drop_index)   \n",
    "        return pfall\n",
    "    except Exception as e:\n",
    "        print(\"does not exist:\" + line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in filetimestamps:\n",
    "    pfall = getencodedpfall(line)\n",
    "    pfall.to_parquet('/tmp/sla_' + line + '.parquet', engine='pyarrow', compression='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
      "blockmgr-cbbf292a-2a1f-4307-9f5a-ba669ad6f595\n",
      "hsperfdata_jovyan\n",
      "hsperfdata_root\n",
      "liblz4-java-11720395175186768582.so\n",
      "liblz4-java-11720395175186768582.so.lck\n",
      "pip-ephem-wheel-cache-rm3pmrj_\n",
      "pip-install-dcrnyug8\n",
      "pip-req-tracker-6ffcown5\n",
      "sla_1580137124017.parquet\n",
      "sla.parquet\n",
      "snappy-1.1.8-922fdeea-d9f2-479a-826e-363bf94a577c-libsnappyjava.so\n",
      "spark-5403d47d-c602-47f0-95f3-f5538619f536\n",
      "spark-f205c30c-6a0c-46ff-bf4d-e0f483a8b196\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
