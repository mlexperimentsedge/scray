{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dfBasics\n",
    "import common\n",
    "import encoder\n",
    "import pfAdapt\n",
    "import charts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lib functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession = dfBasics.getSparkSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sparkSession.read.parquet('hdfs://172.30.17.145:8020/user/admin/stage1/v1/labelencoder/*/*')\n",
    "#.limit(100000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingSchemaids = [ 1480883705,  -404024316,  -183769575,  2031641327, -1576843338,\n",
    "                     -660710506,  -208259125,  -650561809,   -39609584, -1115728007]\n",
    "\n",
    "df = sparkSession.read.parquet('hdfs://172.30.17.145:8020/user/admin/stage1/flat_*/*') \n",
    "df = df.filter(~df['schemaid'].isin(*missingSchemaids)) \\\n",
    "       .withColumn('timestamp', col('timestamp').cast('long')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sparkSession.read.parquet('hdfs://172.30.17.145:8020/sla_sql_data/*/*') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sparkSession.read.parquet('hdfs://172.30.17.145:8020/user/admin/stage1/flat_' + str(cycle) + '/*') \\\n",
    "    .withColumn('timestamp', col('timestamp').cast('long')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMissingSchemaids(pf,hashes):\n",
    "    a = np.array([])\n",
    "    for hash in hashes:  \n",
    "        if len(pf[pf['hashvalue'] == hash]) != len(pd.unique(pf[pf['hashvalue'] == hash]['timestamp'])):\n",
    "            value = pd.unique(pf[pf['hashvalue'] == hash]['schemaid'])\n",
    "            if (value in a) == False:\n",
    "                a = np.append(a,value)\n",
    "    return a.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfcolumnSeries(column):\n",
    "    return df.select(column).dropDuplicates()\n",
    "def dfcolumnCount(column):\n",
    "    return dfcolumnSeries(column).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "def eval1(filteredrows):\n",
    "    element = staticfeatureset.index[0]\n",
    "    res = filteredrows[element].eq(staticfeatureset[element])\n",
    "\n",
    "    for element in staticfeatureset.index:\n",
    "        res = reduce(operator.and_,(res,filteredrows[element].eq(staticfeatureset[element])))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out variable columns and type of value\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "selected = ['timestamp', 'value','year','month', 'day', 'hour', 'minute']\n",
    "\n",
    "def getDFvalueWithHash(currenthash):\n",
    "    pfm = df.filter(col(\"hashvalue\") == currenthash) \\\n",
    "            .withColumn('value', col('value').cast('int'))\\\n",
    "            .select(selected).toPandas().sort_values('timestamp').reset_index() \n",
    "    return pfm\n",
    "\n",
    "def getDFvalueWithHashSchema(schemaid,currenthash):\n",
    "    pfm = df.filter(col(\"hashvalue\") == currenthash) \\\n",
    "            .filter(col(\"schemaid\") == schemaid) \\\n",
    "            .withColumn('value', col('value').cast('int'))\\\n",
    "            .select(selected).toPandas().sort_values('timestamp').reset_index() \n",
    "    return pfm\n",
    "\n",
    "def getIgroupHashes(igroup):\n",
    "        return df.filter(col(\"igroup\") == igroup).select([\"hashvalue\"]).dropDuplicates().toPandas()['hashvalue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns which are variable to find related metrics\n",
    "def getStaticFeatures(_hash):\n",
    "    allcolumns = filteredrows[filteredrows['hashvalue'] == _hash].columns\n",
    "    skipcolumns = ['timestamp', 'value',  'hashvalue', 'igroup', 'inode', 'year', 'month', 'day', 'hour', 'minute']\n",
    "    featurecolumns = allcolumns[~allcolumns.isin(skipcolumns)]\n",
    "    featurecolumns = pfAdapt.getVariableUniqueColums(filteredrows[featurecolumns])\n",
    "    staticfeatureset = filteredrows[filteredrows['hashvalue'] == _hash][featurecolumns].iloc[0]\n",
    "    return staticfeatureset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row1 = df.agg({\"timestamp\": \"min\"}).collect()[0]\n",
    "row2 = df.agg({\"timestamp\": \"max\"}).collect()[0]\n",
    "\n",
    "dfminTimestamp = row1[\"min(timestamp)\"]\n",
    "dfmaxTimestamp = row2[\"max(timestamp)\"]\n",
    "dfrowCount     = df.count()\n",
    "dfcolumnCount  = len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('min:',common.date(dfminTimestamp), 'max:', common.date(dfmaxTimestamp))\n",
    "print('rows:',dfrowCount,'columns:',dfcolumnCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.limit(1).toPandas().columns\n",
    "withoutColumns = ['timestamp', 'value','schemaid', 'hashvalue']\n",
    "columns = columns[~columns.isin(withoutColumns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "schemas = dfcolumnSeries('schemaid').toPandas()['schemaid']\n",
    "igroups = dfcolumnSeries('igroup').toPandas()['igroup']\n",
    "\n",
    "schemaidCount  = dfcolumnCount('schemaid')\n",
    "hashvalueCount = dfcolumnCount('hashvalue')\n",
    "igroupCount = dfcolumnCount('igroup')\n",
    "print('schemaidCount: ',schemaidCount,'hashvalueCount: ',hashvalueCount,'igroupCount: ',igroupCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall = df.limit(100000).toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = getIgroupHashes(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_hash = hashes[10]\n",
    "staticfeatureset = getStaticFeatures(_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relatedhashes = pd.unique(pfall[eval1(pfall)]['hashvalue'])\n",
    "relatedhashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataForRelatedHashes(relatedhashes):\n",
    "    data2 = pd.DataFrame()\n",
    "    start = True\n",
    "    for hash_ in relatedhashes:\n",
    "        c1 =  getDFvalueWithHash(int(hash_))\n",
    "        if start:\n",
    "            data2['date'] = c1['timestamp']\n",
    "            start = False\n",
    "        data2[str(hash_)] =  c1['value']\n",
    "    return data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getDataForRelatedHashes(relatedhashes)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#sns.lineplot(data=data2.iloc[:, :2])\n",
    "sns.lineplot(data=data.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredrows = pfall[pfall['igroup'] == 6]\n",
    "\n",
    "len(filteredrows)\n",
    "hashes = pd.unique(filteredrows['hashvalue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "getMissingSchemaids(pfall,hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.DataFrame()\n",
    "#pfm = getDFvalueWithHash(int(relatedhashes[0]))\n",
    "#\n",
    "data2['date'] = dt['timestamp'].astype(str)\n",
    "data2['value'] = dt['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hashes_d = dfcolumnSeries('hashvalue').toPandas()['hashvalue']\n",
    "hashes_d = df.select(['hashvalue','schemaid']).dropDuplicates().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.unique(hashes_d['hashvalue']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(pd.unique(hashes_d))\n",
    "\n",
    "\n",
    "dt = getDFvalueWithHash(int(hashes_d['schemaid'][0]),int(hashes_d['hashvalue'][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check, if it is unique\n",
    "len(dt['timestamp']), len(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.unique(pfall['hashvalue']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hashes))\n",
    "len(pd.unique(pfall['hashvalue']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getInfoForSchemas(filteredrows)\n",
    "#len(pd.unique(filteredrows['__name__']))\n",
    "#getEncoder('__name__').inverse_transform(pd.unique(filteredrows['__name__']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall[(pfall['schemaid'] == 1629035211) & (pfall['timestamp'] == 1587819506)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getEncoder('igroup').inverse_transform(igroups)\n",
    "for igroup in igroups:\n",
    "    filteredrows = pfall[pfall['igroup'] == igroup]\n",
    "    count = len(pd.unique(filteredrows['inode']))\n",
    "    if count > 1:\n",
    "        print(getEncoder('igroup').inverse_transform([igroup]),igroup,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hash_ in hashes:\n",
    "    metric = filteredrows[filteredrows['hashvalue']==hash_]\n",
    "    count = len(pd.unique(metric['value']))\n",
    "    if count > 3:\n",
    "        print(hash_,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#continue with hash\n",
    "#_hash=-2018833881\n",
    "_hash=-283862276"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!! missing schemas\n",
    "getMissingSchemaids(pfall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredrows[filteredrows['hashvalue'] == _hash]\n",
    "filteredrows[filteredrows['hashvalue'] == _hash][featurecolumns].iloc[0]['__name__']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#['schemaid','igroup']\n",
    "\n",
    "#filteredrows[featurecolumns].head()\n",
    "#getVariableUniqueColums(filteredrows[featurecolumns])\n",
    "\n",
    "# there are still too many columns for _hash\n",
    "#len(getVariableUniqueColums(filteredrows[featurecolumns]))\n",
    "#len(featurecolumns)\n",
    "\n",
    "\n",
    "# there are no variable columns left\n",
    "#getVariableUniqueColums(filteredrows[filteredrows['hashvalue'] == _hash][featurecolumns])\n",
    "\n",
    "# static feature values for metric _hash\n",
    "\n",
    "#staticfeatureset[featurecolumns[1]]\n",
    "#staticfeatureset\n",
    "\n",
    "\n",
    "#featurecolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currenthash     = int(relatedhashes[1])\n",
    "currentfeatures = filteredrows[filteredrows['hashvalue'] == currenthash].iloc[0]\n",
    "currentfeatures['igroup'],currentfeatures['inode'],currentfeatures['hashvalue'],getEncoder('__name__').inverse_transform([currentfeatures['__name__']])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data2 = data2.sort_values('date').reset_index()\n",
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = createLineplot(data2,16,10,1.4,title=\"\",skip=500)\n",
    "label(ax,10,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredrows[featurecolumns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out variable columns and type of value\n",
    "#getVariableUniqueColums(pfm)\n",
    "#selected = ['timestamp', 'value','year','month', 'day', 'hour', 'minute']\n",
    "\n",
    "#which inode is used by metri hashvalue\n",
    "used_inode = filteredrows[filteredrows['hashvalue'] == _hash]['inode'].iloc[0]\n",
    "\n",
    "# other inodes for the igroup\n",
    "inodes_igroup = pd.unique(filteredrows[filteredrows['igroup'] == 6]['inode'])\n",
    "otherinodes = inodes_igroup[inodes_igroup!=used_inode]\n",
    "\n",
    "#\n",
    "pd.unique(filteredrows[filteredrows['igroup'] == 6]['__name__'])\n",
    "pd.unique(filteredrows[(filteredrows['igroup'] == 6) & (filteredrows['__name__'] == 309)]['logical_system'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredrows[filteredrows['inode'] == inodes_igroup[0]]['hashvalue']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "metric = filteredrows[filteredrows['hashvalue']==_hash]\n",
    "#metric = filteredrows[filteredrows['hashvalue']==hashes[0]]\n",
    "#usedcolumns(metric)\n",
    "\n",
    "keep = getVariableUniqueColums(metric)\n",
    "#keepcolumns(dataall,keep)\n",
    "#keep\n",
    "len(metric),len(pd.unique(metric['value'])), pd.unique(metric['value'])\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall1 = pfall[pfall['schemaid'] == schemas[0]]\n",
    "mdcountsall = pfall1.groupby(['igroup','hashvalue'])\n",
    "#['igroup'].count() \n",
    "#data2 = pd.DataFrame()\n",
    "mdcountsall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getEncoder('__name__').inverse_transform([pfall1.iloc[0]['__name__']])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createData(pfall,month,outcome) :\n",
    "    if outcome < 2 :\n",
    "        mdcountsall = pfall[(pfall['month'] == month) & (pfall['outcome'] == outcome)].groupby(['year','month','day','hour'])['outcome'].count()\n",
    "    else :\n",
    "        if (month > 0) & (month < 13) :\n",
    "            mdcountsall = pfall[(pfall['month'] == month)].groupby(['year','month','day','hour'])['outcome'].count()\n",
    "        else :\n",
    "            mdcountsall = pfall.groupby(['year','month','day','hour'])['outcome'].count()    \n",
    "    a,b,c,d = get_ymdh(mdcountsall)\n",
    "    data2 = pd.DataFrame()\n",
    "    data2['date'] = get_ymdh_string(a,b,c,d)\n",
    "    data2['outcome'] =  mdcountsall.reset_index()['outcome'].astype(int) \n",
    "\n",
    "    #for pivot table\n",
    "    data2['hours'] =  d.astype(int) \n",
    "    data2['days']  =  c.astype(int) \n",
    "    piv = pd.pivot_table(data2, values=\"outcome\",index=[\"hours\"], columns=[\"days\"], fill_value=0)\n",
    "    return data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
