{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import time\n",
    "import datetime as dt\n",
    "\n",
    "def adddatecolumns(data,pf,column) :\n",
    "    data['year'] = pf[column].apply(lambda x: x.date().year)\n",
    "    data['month'] = pf[column].apply(lambda x: x.date().month)\n",
    "    data['day'] = pf[column].apply(lambda x: x.date().day)\n",
    "    data['hour'] = pf[column].apply(lambda x: x.time().hour)\n",
    "    data['minute'] = pf[column].apply(lambda x: x.time().minute)\n",
    "    #data['second'] = pf[column].apply(lambda x: x.time().second)\n",
    "    #data['microsecond'] = pf[column].apply(lambda x: x.time().microsecond)\n",
    "\n",
    "def converttimestampcolumnn(pf,tsc) :\n",
    "    pf[tsc] = pf[tsc].apply(lambda x: dt.datetime.fromtimestamp(float(x) / 1e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup charts\n",
    "import pandas as pd\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "print(\"Setup Complete\")\n",
    "\n",
    "#vis functions\n",
    "\n",
    "def label(graph,skip,rot) :\n",
    "    for ind, label in enumerate(graph.get_xticklabels()):\n",
    "        if ind % skip == 0:  # every 10th label is kept\n",
    "            label.set_visible(True)\n",
    "            label.set_rotation(rot)\n",
    "        else:\n",
    "            label.set_visible(False)\n",
    "            \n",
    "def abc(mdcountsall) :\n",
    "    a = mdcountsall.index.get_level_values(0).astype(str)\n",
    "    b = mdcountsall.index.get_level_values(1).astype(str)\n",
    "    c = mdcountsall.index.get_level_values(2).astype(str)\n",
    "    return a + \"-\" + b + \"-\" + c \n",
    "\n",
    "def abcd(mdcountsall) :\n",
    "    return abc(mdcountsall) + \"-\" + mdcountsall.index.get_level_values(3).astype(str)\n",
    "\n",
    "def ymdh(mdcountsall) :\n",
    "    return abcd(mdcountsall)\n",
    "\n",
    "def get_ymdh(mdcountsall) :\n",
    "    a = mdcountsall.index.get_level_values(0).astype(str)\n",
    "    b = mdcountsall.index.get_level_values(1).astype(str)\n",
    "    c = mdcountsall.index.get_level_values(2).astype(str)\n",
    "    d = mdcountsall.index.get_level_values(3).astype(str)\n",
    "    return a,b,c,d\n",
    "\n",
    "def get_ymdh_string(a,b,c,d) :\n",
    "    return a + \"-\" + b + \"-\" + c + \"-\" + d\n",
    "\n",
    "def get_ym(mdcountsall) :\n",
    "    a = mdcountsall.index.get_level_values(0).astype(str)\n",
    "    b = mdcountsall.index.get_level_values(1).astype(str)\n",
    "    return a,b\n",
    "\n",
    "def get_ym_string(a,b) :\n",
    "    return a + \"-\" + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#mdcountsall=pfall[(pfall['month'] == 2) & (pfall['day'] > 16)].groupby(['year','month','day','hour'])['outcome'].count()\n",
    "#mdcountsall=pfall[(pfall['month'] == 2)].groupby(['year','month','day','hour'])['outcome'].count()\n",
    "#mdcountsall=pfall.groupby(['year','month','day'])['outcome'].count()\n",
    "\n",
    "def createData(pfall,month,outcome) :\n",
    "    if outcome < 2 :\n",
    "        mdcountsall = pfall[(pfall['month'] == month) & (pfall['outcome'] == outcome)].groupby(['year','month','day','hour'])['outcome'].count()\n",
    "    else :\n",
    "        if (month > 0) & (month < 13) :\n",
    "            mdcountsall = pfall[(pfall['month'] == month)].groupby(['year','month','day','hour'])['outcome'].count()\n",
    "        else :\n",
    "            mdcountsall = pfall.groupby(['year','month','day','hour'])['outcome'].count()    \n",
    "    a,b,c,d = get_ymdh(mdcountsall)\n",
    "    data2 = pd.DataFrame()\n",
    "    data2['date'] = get_ymdh_string(a,b,c,d)\n",
    "    data2['outcome'] =  mdcountsall.reset_index()['outcome'].astype(int) \n",
    "\n",
    "    #for pivot table\n",
    "    data2['hours'] =  d.astype(int) \n",
    "    data2['days']  =  c.astype(int) \n",
    "    return data2\n",
    "\n",
    "def createData_ym(pfall,month,outcome) :\n",
    "    if outcome < 2 :\n",
    "        mdcountsall = pfall[(pfall['outcome'] == outcome)].groupby(['year','month'])['outcome'].count()\n",
    "    else :\n",
    "        if (month > 0) & (month < 13) :\n",
    "            mdcountsall = pfall[(pfall['month'] == month)].groupby(['year','month'])['outcome'].count()\n",
    "        else :\n",
    "            mdcountsall = pfall.groupby(['year','month'])['outcome'].count()    \n",
    "    a,b = get_ym(mdcountsall)\n",
    "    data2 = pd.DataFrame()\n",
    "    data2['date'] = get_ym_string(a,b)\n",
    "    data2['outcome'] =  mdcountsall.reset_index()['outcome'].astype(int) \n",
    "\n",
    "    #for pivot table\n",
    "    #data2['hours'] =  d.astype(int) \n",
    "    #data2['days']  =  c.astype(int) \n",
    "    return data2\n",
    "\n",
    "## heatmap\n",
    "def createHeatmap(piv,title=\"\") :\n",
    "    plt.figure(figsize=(24,8))\n",
    "    plt.title(title)\n",
    "    ax = sns.heatmap(piv, square=True)\n",
    "    plt.setp( ax.xaxis.get_majorticklabels(), rotation=0 )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return ax\n",
    "\n",
    "def createBarplot(md,fx,fy,fontscale,title=\"\") :\n",
    "    sns.set(style='whitegrid', palette='muted', font_scale=fontscale)\n",
    "    plt.figure(figsize=(fx,fy))\n",
    "    plt.title(title)\n",
    "    ax = sns.barplot(x=md['date'], y=md['outcome'], data=md)\n",
    "    plt.setp( ax.xaxis.get_majorticklabels(), rotation=0 )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return ax\n",
    "\n",
    "# kernel density estimate (KDE) \n",
    "def createKDE(data2,fx,fy,fontscale,title=\"\") :\n",
    "    sns.set(style='whitegrid', palette='muted', font_scale=fontscale)\n",
    "    plt.figure(figsize=(fx,fy))\n",
    "    plt.title(title)\n",
    "    # Histogram \n",
    "    #ax = sns.distplot(a=data2['outcome'], kde=False)\n",
    "    ax = sns.kdeplot(data=data2['outcome'], shade=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLineplot(md,fx,fy,fontscale,title=\"\",skip=0,rot=90) :\n",
    "    sns.set(style='whitegrid', palette='muted', font_scale=fontscale)\n",
    "    plt.figure(figsize=(fx,fy))\n",
    "    plt.title(title)\n",
    "    ax = sns.lineplot(x=md['date'], y=md['value'], data=md)\n",
    "    for ind, label in enumerate(ax.get_xticklabels()):\n",
    "        if ind % skip == 0:  # every 10th label is kept\n",
    "            label.set_visible(True)\n",
    "            label.set_rotation(rot)\n",
    "        else:\n",
    "            label.set_visible(False)\n",
    "    #plt.setp( ax.xaxis.get_majorticklabels(), rotation=90 )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return ax\n",
    "\n",
    "#ax = createLineplot(pfall,16,10,1.4,title=\"\")\n",
    "#label(ax,500,80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series Anomaly Detection with LSTM Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 22, 10\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "OUTCOME = 'close'\n",
    "\n",
    "TIME_STEPS = 24\n",
    "#TIME_STEPS = 30\n",
    "#TIME_STEPS = 720\n",
    "#TIME_STEPS = 168\n",
    "#TIME_STEPS = 336\n",
    "\n",
    "# setup data (current)\n",
    "def createDataframe(pfall) :\n",
    "    data3 = createData(pfall,0,2)\n",
    "    df = pd.DataFrame()\n",
    "    df[OUTCOME] = data3['outcome']\n",
    "    df.set_index(data3['date'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def getTrainAndTest(df,TRAIN_SIZE) :\n",
    "    train_size = int(len(df) * TRAIN_SIZE)\n",
    "    test_size = len(df) - train_size\n",
    "    train, test = df.iloc[0:train_size], df.iloc[train_size:len(df)]\n",
    "    print(\"train.shape: \",train.shape, \"test.shape: \", test.shape)\n",
    "    return train, test\n",
    "\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)        \n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def initmodel():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.LSTM(\n",
    "        units=64, \n",
    "        input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "    ))\n",
    "    model.add(keras.layers.Dropout(rate=0.2))\n",
    "    model.add(keras.layers.RepeatVector(n=X_train.shape[1]))\n",
    "    model.add(keras.layers.LSTM(units=64, return_sequences=True))\n",
    "    model.add(keras.layers.Dropout(rate=0.2))\n",
    "    model.add(keras.layers.TimeDistributed(keras.layers.Dense(units=X_train.shape[2])))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def testScoreDF(model, THRESHOLD) : \n",
    "    X_test_pred = model.predict(X_test)\n",
    "    test_mae_loss = np.mean(np.abs(X_test_pred - X_test), axis=1)\n",
    "\n",
    "    test_score_df = pd.DataFrame(index=test[TIME_STEPS:].index)\n",
    "    test_score_df['loss'] = test_mae_loss\n",
    "    test_score_df['threshold'] = THRESHOLD\n",
    "    test_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold\n",
    "    test_score_df[OUTCOME] = test[TIME_STEPS:][OUTCOME]\n",
    "    return test_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingSchemaids = [ 1480883705,  -404024316,  -183769575,  2031641327, -1576843338,\n",
    "                     -660710506,  -208259125,  -650561809,   -39609584, -1115728007]\n",
    "\n",
    "def withoutMissingSchemas(pf):\n",
    "    return pf[~pf['schemaid'].isin(missingSchemaids)]\n",
    "\n",
    "def getHashValues(pf):\n",
    "    return pd.unique(pf['hashvalue'])\n",
    "\n",
    "# max timestamp for splits\n",
    "def getMaxTimestamp(pf):\n",
    "    return pf.loc[pf['timestamp'].idxmax()]['timestamp']\n",
    "\n",
    "def getCountDF(pf,column,hashes):\n",
    "    dft = pd.DataFrame(columns=[column, 'count'])\n",
    "    i=0\n",
    "    for hash in hashes:\n",
    "        pfall=pf[pf[column] == hash]\n",
    "        num=len(pd.unique(pfall['value']))\n",
    "        dft.loc[i] = [hash] + [num]\n",
    "        i=i+1\n",
    "    return dft.sort_values('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "#sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "\n",
    "#sc = SparkContext()\n",
    "#sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "sparkSession = SparkSession.builder.config('spark.local.dir', '/tmp').config(\"spark.executor.memory\", \"8g\").config(\"spark.driver.memory\", \"8g\").config(\"spark.driver.maxResultSize\", \"0\").appName(\"example-pyspark-read-and-write\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sparkSession.read.parquet('hdfs://172.30.17.145:8020/user/admin/stage1/flat_7/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show()\n",
    "pf = df.limit(1000000).dropDuplicates().orderBy('timestamp').toPandas()\n",
    "getMaxTimestamp(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1961237651 , 632085806\n",
    "#pf[(pf['hashvalue'] == 1961237651) & (pf['timestamp'] == 1583856953)] \n",
    "pf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = pf[(pf['hashvalue'] == 632085806) & (pf['timestamp'] == 1583856953)] \n",
    "tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMissingSchemaids(pf):\n",
    "    a = np.array([])\n",
    "    for hash in hashes:  \n",
    "        if len(pf[pf['hashvalue'] == hash]) != len(pd.unique(pf[pf['hashvalue'] == hash]['timestamp'])):\n",
    "            value = pd.unique(pf[pf['hashvalue'] == hash]['schemaid'])\n",
    "            if (value in a) == False:\n",
    "                a = np.append(a,value)\n",
    "    return a.astype(int)\n",
    "\n",
    "def usedcolumns(tb):\n",
    "    col = []\n",
    "    for column in tb.columns:\n",
    "        if tb.iloc[0][column] != None :\n",
    "            col.append(column)\n",
    "    return col\n",
    "\n",
    "def keepcolumns(tb,keep):\n",
    "    for column in tb.columns:\n",
    "        if column not in keep :\n",
    "            del(tb[column])\n",
    "\n",
    "            \n",
    "            \n",
    "#used = usedcolumns(tb)\n",
    "#testdel(tb,used)\n",
    "#tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pf.head()\n",
    "#pf[pf['hashvalue'] == 1636706336].head()\n",
    "#,pf[pf['hashvalue'] == -1407530683].iloc[1]\n",
    "len(pd.unique(pf['hashvalue']))\n",
    "hashes = pd.unique(pf['hashvalue'])\n",
    "\n",
    "#len(pf[pf['hashvalue'] == hashes[5]]),len(pd.unique(pf[pf['hashvalue'] == hashes[5]]['timestamp']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf['schemaid'] = pf['schemaid'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "  \n",
    "#missingSchemaids = getMissingSchemaids(pf)  \n",
    "#missingSchemaids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = withoutMissingSchemas(pf)\n",
    "hashes = getHashValues(pf)\n",
    "counts = getCountDF(pf,hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMaxTimestamp(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "cat_features\n",
    "dataall['service'] = dataall['service'].astype(str)\n",
    "dataall['__name__'] = dataall['__name__'].astype(str)\n",
    "dataall['instance'] = dataall['instance'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = encodeall(dataall,cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall=pf[pf['hashvalue'] == dft.loc[13844]['hashvalue']].reset_index()\n",
    "pfall['value'] = pfall['value'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash=360972192\n",
    "len(pf[pf['hashvalue'] == hash]),len(pd.unique(pf[pf['hashvalue'] == hash]['timestamp']))\n",
    "pf[pf['hashvalue'] == hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/50041551/tell-labelenocder-to-ignore-new-labels\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "\n",
    "class TolerantLabelEncoder(LabelEncoder):\n",
    "    def __init__(self, ignore_unknown=False,\n",
    "                       unknown_original_value='unknown', \n",
    "                       unknown_encoded_value=-1):\n",
    "        self.ignore_unknown = ignore_unknown\n",
    "        self.unknown_original_value = unknown_original_value\n",
    "        self.unknown_encoded_value = unknown_encoded_value\n",
    "\n",
    "    def transform(self, y):\n",
    "        check_is_fitted(self, 'classes_')\n",
    "        y = column_or_1d(y, warn=True)\n",
    "\n",
    "        indices = np.isin(y, self.classes_)\n",
    "        if not self.ignore_unknown and not np.all(indices):\n",
    "            raise ValueError(\"y contains new labels: %s\" \n",
    "                                         % str(np.setdiff1d(y, self.classes_)))\n",
    "\n",
    "        y_transformed = np.searchsorted(self.classes_, y)\n",
    "        y_transformed[~indices]=self.unknown_encoded_value\n",
    "        return y_transformed\n",
    "\n",
    "    def inverse_transform(self, y):\n",
    "        check_is_fitted(self, 'classes_')\n",
    "\n",
    "        labels = np.arange(len(self.classes_))\n",
    "        indices = np.isin(y, labels)\n",
    "        if not self.ignore_unknown and not np.all(indices):\n",
    "            raise ValueError(\"y contains new labels: %s\" \n",
    "                                         % str(np.setdiff1d(y, self.classes_)))\n",
    "\n",
    "        y_transformed = np.asarray(self.classes_[y], dtype=object)\n",
    "        y_transformed[~indices]=self.unknown_original_value\n",
    "        return y_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEncoders(dataall,columns):\n",
    "    for column in columns:\n",
    "        le = TolerantLabelEncoder(ignore_unknown=True)\n",
    "        #le.fit([1, 2, 2, 6])\n",
    "        le.fit(dataall[column])\n",
    "        LabelEncoder()\n",
    "        print(le.classes_)\n",
    "        np.save(column + '.npy', le.classes_)\n",
    "        \n",
    "def encode(dataall,columns):\n",
    "    # save np.load\n",
    "    np_load_old = np.load\n",
    "\n",
    "    # modify the default parameters of np.load\n",
    "    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "    for column in columns:\n",
    "        encoder = TolerantLabelEncoder(ignore_unknown=True)\n",
    "        encoder.classes_ = np.load(column + '.npy')\n",
    "        dataall[column] = encoder.transform(dataall[column]) \n",
    "\n",
    "    # restore np.load for future normal usage\n",
    "    np.load = np_load_old\n",
    "    \n",
    "def getEncoder(column):\n",
    "    # save np.load\n",
    "    np_load_old = np.load\n",
    "\n",
    "    # modify the default parameters of np.load\n",
    "    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "    encoder = TolerantLabelEncoder(ignore_unknown=True)\n",
    "    encoder.classes_ = np.load(column + '.npy')\n",
    "        \n",
    "    # restore np.load for future normal usage\n",
    "    np.load = np_load_old\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split instance name\n",
    "\n",
    "def igroup(x):\n",
    "    if x[0:3] == 'cls':\n",
    "        res = x.split('0')\n",
    "        return res[0]\n",
    "    return x\n",
    "    \n",
    "\n",
    "def inode(x):\n",
    "    if x[0:3] == 'cls':\n",
    "        res = x.split('0')\n",
    "        return res[1].split(':')[0]\n",
    "    return '1'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "le = LabelEncoder()\n",
    "#le.fit([1, 2, 2, 6])\n",
    "le.fit(dataall['adapter'])\n",
    "LabelEncoder()\n",
    "le.classes_\n",
    "#np.save('classes.npy', le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le.transform(dataall['adapter']) \n",
    "#dataall['adapter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = np.load('classes.npy')\n",
    "encoder.transform([1, 1, 2, 6,6,1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "cat_features = ['service', '__name__','instance']\n",
    "\n",
    "def encodeall(pfall,cat_features):\n",
    "    #Prepping categorical variables\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    encoder = LabelEncoder()\n",
    "    # Apply the label encoder to each column\n",
    "    encodedpfall = pfall[cat_features].apply(encoder.fit_transform)\n",
    "    return encodedpfall\n",
    "  \n",
    "def getpfbasic(pfall) :   \n",
    "    #pfall = df2.limit(5000000).toPandas()    \n",
    "    #print(pfall.loc[pfall['CSTARTTIME'].idxmax()]['CSTARTTIME'],len(pfall.index))\n",
    "    pfall = pfall.assign(outcome=(~( ((pfall['CSTATUS'] == 'PENDING') & (pfall['CSERVICE'] == 'InvoicePortal')) | ((pfall['CSTATUS'] == 'PENDING') & (pfall['CSERVICE'] == 'IDS')) | (pfall['CSTATUS'] == 'SUCCESS') | (pfall['CSTATUS'] == 'SUCCESS_DOWNLOADED') | (pfall['CSTATUS'] == 'SUCCESS_POLLQUEUE'))).astype(int))\n",
    "    converttimestampcolumnn(pfall,'CSTARTTIME')\n",
    "    pfall['CGLOBALMESSAGEID'] = pfall['CGLOBALMESSAGEID'].apply(hash)\n",
    "    #pfall['CSENDERENDPOINTID'] = pfall['CSENDERENDPOINTID'].astype(str)   \n",
    "    #pfall['CRECEIVERENDPOINTID'] = pfall['CRECEIVERENDPOINTID'].astype(str)\n",
    "    pfall['CMESSAGETAT2'] = pfall['CMESSAGETAT2'].astype(int)\n",
    "    pfall['CSLATAT'] = pfall['CSLATAT'].astype(int)\n",
    "    pfall['CINBOUNDSIZE'] = pfall['CINBOUNDSIZE'].astype(int)\n",
    "    return pfall\n",
    "\n",
    "def getpf(df2) :    \n",
    "    pfall = getpfbasic(df2)\n",
    "    encodedpfall = encodeall(pfall,cat_features)\n",
    "    dataall = pfall[['CGLOBALMESSAGEID','CMESSAGETAT2','CSLATAT','CINBOUNDSIZE', 'outcome']].join(encodedpfall)\n",
    "    adddatecolumns(dataall,pfall)    \n",
    "    return dataall\n",
    "\n",
    "def astype(pfall,selected,newtype):\n",
    "    for each in selected:\n",
    "        pfall[each] = pfall[each].astype(newtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "\n",
    "selected = ['date', 'timestamp', 'value', 'product', 'service', '__name__','instance',  'schemaid', 'hashvalue']\n",
    "#selected = ['date','timestamp','value','product','service','__name__','instance','job','le','port','schemaid','hashvalue']\n",
    "#selected = ['timestamp', 'value', 'schemaid', 'hashvalue']\n",
    "\n",
    "def getpfall(df,selected) :\n",
    "    #pfall = df.limit(5000000).toPandas()  \n",
    "    pfall = df.toPandas() \n",
    "    pfall['timestamp'] = pfall['timestamp'].astype(int)\n",
    "    pfall['schemaid'] = pfall['schemaid'].astype(int)\n",
    "    pfall['hashvalue'] = pfall['hashvalue'].astype(int)\n",
    "    #pfall['value'] = pfall['value'].astype(float)\n",
    "    #for each in selected:\n",
    "    #    pfall[each] = pfall[each].astype(str)\n",
    "    if len(pfall) == 0:\n",
    "        return pfall,0,0\n",
    "    #used = usedcolumns(pfall)\n",
    "    #keepcolumns(pfall,used)\n",
    "    return pfall, int(pfall.loc[pfall['timestamp'].astype(int).idxmax()]['timestamp']),len(pfall.index)        \n",
    "\n",
    "def gettest(to,selected) :\n",
    "    df2 = df.withColumn('timestamp', col('timestamp').cast('long')).filter(col(\"timestamp\") < to ).select(selected).dropDuplicates().orderBy('timestamp')    \n",
    "    return getpfall(df2,selected)\n",
    "\n",
    "def getdata_lt(to,selected=[], filter_names = \"\") :\n",
    "    if len(filter_names) > 0:\n",
    "        print (len(filter_names))\n",
    "        df2 = df.filter(col(\"__name__\") == filter_names ).withColumn('timestamp', col('timestamp').cast('long')).filter(col(\"timestamp\") < to ).dropDuplicates().orderBy('timestamp') \n",
    "    else:\n",
    "        df2 = df.withColumn('timestamp', col('timestamp').cast('long')).filter(col(\"timestamp\") < to ).dropDuplicates().orderBy('timestamp')  \n",
    "    #df2 = df.filter(col(\"__name__\") == 'bis_adapter_finished_processes_total' ).withColumn('timestamp', col('timestamp').cast('long')).filter(col(\"timestamp\") < to ).select(selected).dropDuplicates().orderBy('timestamp') \n",
    "    #dataall = getpf(df2)    \n",
    "    #return dataall\n",
    "    return getpfall(df2,selected)\n",
    "    #return df2\n",
    "    \n",
    "\n",
    "# >=from  <=to    \n",
    "def getdata_ft(_from,_diff,selected=[], filter_names = \"\") :\n",
    "    to = _from + _diff\n",
    "    if len(filter_names) == 0:\n",
    "        print(_from,to)\n",
    "        df2 = df.withColumn('timestamp', col('timestamp').cast('long')) \\\n",
    "                .filter(col(\"timestamp\") >= _from ) \\\n",
    "                .filter(col(\"timestamp\") <= to ) \\\n",
    "                .filter(~df['schemaid'].isin(*missingSchemaids)) \n",
    "                #.orderBy('timestamp') \n",
    "        print((df2.count(), len(df2.columns)))\n",
    "    else:    \n",
    "        df2 = df.filter(col(\"__name__\") == filter_names ).withColumn('timestamp', col('timestamp').cast('long')).filter(col(\"timestamp\") >= _from ).filter(col(\"timestamp\") <= to ).dropDuplicates().orderBy('timestamp') \n",
    "    return getpfall(df2,selected)\n",
    "\n",
    "# >from  <=to   \n",
    "def getdata_gt(_from,_diff,selected=[], filter_names = \"\") :\n",
    "    to = _from + _diff\n",
    "    if len(filter_names) == 0:\n",
    "        print(_from,to)\n",
    "        df2 = df.withColumn('timestamp', col('timestamp').cast('long')) \\\n",
    "                .filter(col(\"timestamp\") > _from ) \\\n",
    "                .filter(col(\"timestamp\") <= to ) \\\n",
    "                .filter(~df['schemaid'].isin(*missingSchemaids)) \n",
    "                #.orderBy('timestamp') \n",
    "        print((df2.count(), len(df2.columns)))\n",
    "    else:    \n",
    "        df2 = df.filter(col(\"__name__\") == filter_names ).withColumn('timestamp', col('timestamp').cast('long')).filter(col(\"timestamp\") >= _from ).filter(col(\"timestamp\") <= to ).dropDuplicates().orderBy('timestamp') \n",
    "    return getpfall(df2,selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected = ['__name__']\n",
    "#df4 = df.select(selected).dropDuplicates().toPandas() \n",
    "#len(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missingSchemaids\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row1 = df.agg({\"timestamp\": \"min\"}).collect()[0]\n",
    "row2 = df.agg({\"timestamp\": \"max\"}).collect()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (row1)\n",
    "dfminTimestamp = row1[\"min(timestamp)\"]\n",
    "print (row2)\n",
    "dfmaxTimestamp = row2[\"max(timestamp)\"]\n",
    "\n",
    "dfrowCount    = df.count()\n",
    "dfcolumnCount = len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_from =  dfminTimestamp\n",
    "to = _from + 100\n",
    "timestamp_diff = 3000\n",
    "\n",
    "df2 = getdata_ft(_from,timestamp_diff,selected=[], filter_names = \"\")\n",
    "\n",
    "#df2.show()\n",
    "#print((df2.count(), len(df2.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withoutColumns = ['date', 'timestamp', 'value','instance','schemaid', 'hashvalue']\n",
    "columns = df.limit(1).toPandas().columns\n",
    "columns = columns[~columns.isin(withoutColumns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import time\n",
    "import datetime as dt\n",
    "import calendar\n",
    "import pytz\n",
    "de = pytz.timezone('Europe/Berlin')\n",
    "\n",
    "def date(x):\n",
    "    return  dt.datetime.fromtimestamp(float(x), tz=de)\n",
    "\n",
    "\n",
    "def adddatecolumns(data,pf,column) :\n",
    "    data['year'] = pf[column].apply(lambda x: date(x).date().year)\n",
    "    data['month'] = pf[column].apply(lambda x: date(x).date().month)\n",
    "    data['day'] = pf[column].apply(lambda x: date(x).date().day)\n",
    "    data['hour'] = pf[column].apply(lambda x: date(x).time().hour)\n",
    "    data['minute'] = pf[column].apply(lambda x: date(x).time().minute)\n",
    "    #data['second'] = pf[column].apply(lambda x: x.time().second)\n",
    "    #data['microsecond'] = pf[column].apply(lambda x: x.time().microsecond)\n",
    "\n",
    "def converttimestampcolumnn(pf,tsc) :\n",
    "    pf[tsc] = pf[tsc].apply(lambda x: dt.datetime.fromtimestamp(float(x) / 1e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeDataframe(dataall):\n",
    "    astype(dataall,columns,str)\n",
    "    encode(dataall,columns)\n",
    "\n",
    "    del dataall['date']\n",
    "    #del dataall['timestamp']\n",
    "    \n",
    "    astype(dataall,['instance'],str)\n",
    "    dataall['igroup'] = dataall['instance'].apply(lambda x: igroup(x))\n",
    "    dataall['inode']  = dataall['instance'].apply(lambda x: inode(x))\n",
    "\n",
    "    #createEncoders(dataall,['igroup'])\n",
    "    encode(dataall,['igroup'])\n",
    "    astype(dataall,['inode'],int)\n",
    "    del dataall['instance']\n",
    "\n",
    "    # convert timestamp to datetime and add column date\n",
    "    import calendar\n",
    "    import pytz\n",
    "    de = pytz.timezone('Europe/Berlin')\n",
    "    #dataall['date'] = dataall['timestamp'].apply(lambda x: dt.datetime.fromtimestamp(float(x), tz=de))\n",
    "    adddatecolumns(dataall,dataall,'timestamp')\n",
    "    #del dataall['date'] \n",
    "    return dataall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = df.select('timestamp').toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest['date'] = dftest['timestamp'].apply(lambda x: dt.datetime.fromtimestamp(float(x), tz=de))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adddatecolumns(dftest,dftest,'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[0].head()\n",
    "import calendar\n",
    "import pytz\n",
    "de = pytz.timezone('Europe/Berlin')\n",
    "dt.datetime.fromtimestamp(float(df2[0].iloc[0]['timestamp']), tz=de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataall = encodeDataframe(df2[0])\n",
    "timestamp = df2[1]\n",
    "timestamp_diff = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataall = withoutMissingSchemas(dataall)\n",
    "#dataall.head()\n",
    "\n",
    "dataall.to_parquet('/tmp/myfile_7_' + str(timestamp) + '.parquet', engine='fastparquet', compression='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm /tmp/*.parquet\n",
    "#!ls -lht /tmp/*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max timestamp\n",
    "dataall.loc[dataall['timestamp'].idxmax()]['timestamp'], timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk(timestamp,max_cycles=1,timestamp_diff=1000):\n",
    "    cycle = 0\n",
    "    while True:\n",
    "        #next\n",
    "        df2 = getdata_gt(timestamp,timestamp_diff)\n",
    "        encodeDataframe(df2[0])\n",
    "\n",
    "        #merge and prepare for next step\n",
    "        #dataall = dataall.append(df2[0], ignore_index=True)\n",
    "        timestamp = df2[1]\n",
    "        print(timestamp,df2[2])\n",
    "        df2[0].to_parquet('/tmp/myfile_7_' + str(timestamp) + '.parquet', engine='fastparquet', compression='GZIP')\n",
    "        \n",
    "        if df2[2] == 0:\n",
    "            break  \n",
    "        cycle = cycle + 1\n",
    "        if cycle == max_cycles:\n",
    "            break\n",
    "    return timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = 1584187648\n",
    "timestamp = walk(timestamp,1000,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.unique(dataall['hashvalue']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = getdata_ft(timestamp,timestamp_diff,selected=[], filter_names = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(df3[0]['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_lt = 1583856983\n",
    "#start_lt = 1585498272\n",
    "\n",
    "timestamp_diff = 5000\n",
    "dataall2 = getdata_lt(start_lt,selected)\n",
    "dataall = dataall2[0]\n",
    "timestamp = dataall2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestamp_diff = 10\n",
    "def walk(dataall,timestamp,max_cycles=1):\n",
    "    cycle = 0\n",
    "    while True:\n",
    "        dataall2 = getdata_gt(timestamp ,timestamp_diff,selected)\n",
    "        print(dataall2[2])\n",
    "        if dataall2[2] == 0:\n",
    "            break  \n",
    "        dataall = dataall.append(dataall2[0], ignore_index=True)\n",
    "        cycle = cycle + 1\n",
    "        if cycle == max_cycles:\n",
    "            break\n",
    "        timestamp = dataall2[1]\n",
    "    return dataall,dataall2[1],dataall2[2]\n",
    "\n",
    "#timestamp = dataall2[1]\n",
    "dataall,timestamp,length = walk(dataall,timestamp,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_diff = 10\n",
    "def walk(dataall2,dataall,max_cycles=1):\n",
    "    cycle = 0\n",
    "    while True:\n",
    "        dataall2 = getdata_gt(dataall2[1] ,timestamp_diff,selected)\n",
    "        print(dataall2[2])\n",
    "        if dataall2[2] == 0:\n",
    "            break  \n",
    "        dataall = dataall.append(dataall2[0], ignore_index=True)\n",
    "        cycle = cycle + 1\n",
    "        if cycle == max_cycles:\n",
    "            break\n",
    "    return dataall,dataall2[1],dataall2[2]\n",
    "\n",
    "dataall,timestamp,length = walk(dataall2,dataall,1)\n",
    "dataall = dataall2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testEncoding\n",
    "#pd.unique(dataall['__name__'])\n",
    "#astype(dataall,['adapter'],str)\n",
    "#pd.unique(dataall['__name__'])\n",
    "dataall.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataall to dataframe and store it to hdfs\n",
    "#dataall\n",
    "#dfm = sparkSession.createDataFrame(dataall)\n",
    "#dfm.write.parquet('hdfs://172.30.17.145:8020/user/admin/bis_adapter_finished_processes_total_7.parquet')\n",
    "\n",
    "import os\n",
    "#os.environ['http_proxy'] = \"http://172.30.12.56:3128\" \n",
    "#os.environ['https_proxy'] = \"https://172.30.12.56:3128\"    \n",
    "#!conda install -y -c conda-forge fastparquet\n",
    "dataall.to_parquet('/tmp/myfile_7.parquet', engine='fastparquet', compression='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!ls -lh /tmp/myfile*.parquet\n",
    "\n",
    "#pd.read_parquet('/tmp/myfile.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pft = dataall2.toPandas() \n",
    "#dataall\n",
    "#getMaxTimestamp(dataall)\n",
    "#dataall = withoutMissingSchemas(dataall)\n",
    "\n",
    "used = usedcolumns(dataall)\n",
    "#keepcolumns(dataall,used)\n",
    "used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVariableUniqueColums(dataall):\n",
    "    col = []\n",
    "    for column in dataall.columns:\n",
    "        size = len(pd.unique(dataall[column]))\n",
    "        #print(column,size)\n",
    "        if size > 1:\n",
    "            col.append(column)\n",
    "    return col\n",
    "\n",
    "keep = getVariableUniqueColums(dataall)\n",
    "keepcolumns(dataall,keep)\n",
    "keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataall.head()\n",
    "#pd.unique(dataall['adapter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used_columns = ['__name__','adapter','job','logical_system','instance','product','service']\n",
    "used_columns = ['__name__','job','instance','product','service']\n",
    "astype(dataall,used_columns,str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withoutColumns = ['date', 'timestamp', 'value','instance','schemaid', 'hashvalue']\n",
    "columns = df.limit(1).toPandas().columns\n",
    "columns = columns[~columns.isin(withoutColumns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns:\n",
    "    print(column)\n",
    "    df4 = df.select(column).dropDuplicates().toPandas() \n",
    "    df4[column] = df4[column].astype(str)\n",
    "    createEncoders(df4,[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'instance'\n",
    "df4 = df.select(column).dropDuplicates().toPandas() \n",
    "df4[column] = df4[column].astype(str)\n",
    "#createEncoders(df4,[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#astype(dataall,['instance'],str)\n",
    "df4['igroup'] = df4['instance'].apply(lambda x: igroup(x))\n",
    "df4['inode']  = df4['instance'].apply(lambda x: inode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createEncoders(df4,['igroup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = getEncoder(columns[8])\n",
    "print(encoder.inverse_transform([5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = ['__name__']\n",
    "df4 = df.select(selected).dropDuplicates().toPandas() \n",
    "len(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createEncoders(dataall,used_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode(dataall,used_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'bis_adapter_finished_processes_total'\n",
    "#encoder = getEncoder(dataall,'__name__')\n",
    "#encoder = getEncoder('__name__')\n",
    "encoder = getEncoder(used_columns[0])\n",
    "print(encoder.inverse_transform([413]))\n",
    "#print(encoder.transform([f]))\n",
    "#encoder.classes_\n",
    "#createEncoders(dataall,['service'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = TolerantLabelEncoder(ignore_unknown=True)\n",
    "en.fit(['a','b'])\n",
    "\n",
    "print(en.transform(['a', 'c', 'b']))\n",
    "# Output: [ 0 -1  1]\n",
    "\n",
    "print(en.inverse_transform([-1, 0, 1]))\n",
    "# Output: ['unknown' 'a' 'b']\n",
    "\n",
    "en.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('test' + '.npy', en.classes_)\n",
    "getEncoder('test').classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save encoders / cannot adapt to new labels\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "def createEncoders(dataall,columns):\n",
    "    for column in columns:\n",
    "        le = LabelEncoder()\n",
    "        #le.fit([1, 2, 2, 6])\n",
    "        le.fit(dataall[column])\n",
    "        LabelEncoder()\n",
    "        le.classes_\n",
    "        np.save(column + '.npy', le.classes_)\n",
    "    \n",
    "def encode(dataall,columns):\n",
    "    # save np.load\n",
    "    np_load_old = np.load\n",
    "\n",
    "    # modify the default parameters of np.load\n",
    "    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "    for column in columns:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.classes_ = np.load(column + '.npy')\n",
    "        dataall[column] = encoder.transform(dataall[column]) \n",
    "\n",
    "    # restore np.load for future normal usage\n",
    "    np.load = np_load_old\n",
    "    \n",
    "createEncoders(dataall,['__name__','adapter','job','logical_system','product','service'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "encode(dataall,['__name__','adapter','job','logical_system','product','service'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataall = withoutMissingSchemas(dataall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astype(dataall,['instance'],str)\n",
    "dataall['igroup'] = dataall['instance'].apply(lambda x: igroup(x))\n",
    "dataall['inode']  = dataall['instance'].apply(lambda x: inode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createEncoders(dataall,['igroup'])\n",
    "encode(dataall,['igroup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamp to datetime and add column date\n",
    "import calendar\n",
    "import pytz\n",
    "de = pytz.timezone('Europe/Berlin')\n",
    "dataall['date'] = dataall['timestamp'].apply(lambda x: dt.datetime.fromtimestamp(float(x), tz=de))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adddatecolumns(dataall,dataall,'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features=['group']\n",
    "enc = encodeall(dataall,cat_features)\n",
    "dataall['group'] = enc['group']\n",
    "#dataall['instance'] = enc['instance']\n",
    "astype(dataall,['group','node','value'],int)\n",
    "del(dataall['instance']) \n",
    "dataall['service'] = encodeall(dataall,['service'])['service']\n",
    "dataall['logical_system'] = encodeall(dataall,['logical_system'])['logical_system']\n",
    "dataall['job'] = encodeall(dataall,['job'])['job']\n",
    "dataall['adapter'] = encodeall(dataall,['adapter'])['adapter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataall\n",
    "import calendar\n",
    "import pytz\n",
    "res=\"2020-03-10 16:59:59\"\n",
    "x=1583855999\n",
    "de = pytz.timezone('Europe/Berlin')\n",
    "dt.datetime.fromtimestamp(float(x), tz=de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataall.dtypes\n",
    "#dataall.head()\n",
    "dataall[dataall['inode'] == '9']\n",
    "pd.unique(dataall['igroup'])\n",
    "#astype(dataall,'date',dt.datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataall[(dataall['service'] == 3) & (dataall['job'] == 6) & (dataall['adapter'] == 10) & (dataall['inode'] != 4)]['inode']\n",
    "#pd.unique(dataall[dataall['job'] == 'MAKTEST']['service'])\n",
    "#pd.unique(dataall[dataall['job'] == 'MAKPRO']['node'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdcountsall = dataall[(dataall['service'] == 3) & (dataall['job'] == 6) & (dataall['adapter'] == 10) & (dataall['node'] == 3)]\n",
    "mdcountsall = mdcountsall.groupby(['year','month','day','hour'])['value'].count()    \n",
    "len(mdcountsall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = get_ymdh(mdcountsall)\n",
    "data2 = pd.DataFrame()\n",
    "data2['date'] = get_ymdh_string(a,b,c,d)\n",
    "data2['value'] =  mdcountsall.reset_index()['value'].astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = createLineplot(data2,16,10,1.4,title=\"\",skip=10)\n",
    "label(ax,10,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def label(graph,skip,rot) :\n",
    "    for ind, label in enumerate(graph.get_xticklabels()):\n",
    "        if ind % skip == 0:  # every 10th label is kept\n",
    "            label.set_visible(True)\n",
    "            label.set_rotation(rot)\n",
    "        else:\n",
    "            label.set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataall = withoutMissingSchemas(dataall)\n",
    "hashes = getHashValues(dataall)\n",
    "countpf = getCountDF(dataall,'hashvalue',hashes).reset_index()\n",
    "countpf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dataall[dataall['hashvalue'] == countpf.iloc[224]['hashvalue']]\n",
    "\n",
    "ax = createLineplot(b,16,10,1.4,title=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataall[dataall['hashvalue'] == -869302958]\n",
    "row = dataall[dataall['hashvalue'] == countpf.iloc[224]['hashvalue']].iloc[0]\n",
    "\n",
    "cluster = cluster_instances\n",
    "cluster_instances = pd.unique(cluster['instance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.unique(dataall['__name__'])\n",
    "countnames = getCountDF(dataall,'__name__',names).reset_index()\n",
    "countnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.unique(dataall[dataall['__name__'] == 'bis_adapter_finished_processes_total']['schemaid'])\n",
    "pd.unique(dataall[dataall['schemaid'] == 232746291]['__name__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astype(dataall,cat_features,str)\n",
    "enc = encodeall(dataall,cat_features)\n",
    "dataall['service_enc'] = enc['service']\n",
    "dataall['__name___enc'] = enc['__name__']\n",
    "dataall['instance_enc'] = enc['instance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataall[dataall['hashvalue'] == -869302958]\n",
    "row = dataall[dataall['hashvalue'] == -488244161].iloc[0]\n",
    "row['service_enc'], row['__name___enc'], row['instance_enc'],row['schemaid']\n",
    "cluster = dataall[(dataall['service_enc'] == row['service_enc']) &\n",
    "        (dataall['__name___enc'] == row['__name___enc']) &\n",
    "        (dataall['schemaid'] == row['schemaid'])]\n",
    "cluster_instances = pd.unique(cluster['instance_enc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cluster[cluster['instance_enc'] == cluster_instances[0]]\n",
    "pd.unique(a['hashvalue'])\n",
    "#getMissingSchemaids(cluster)\n",
    "#1961237651 , 632085806\n",
    "#cluster[cluster['hashvalue'] == -48824416]\n",
    "\n",
    "#le   = pd.unique(a['le'])\n",
    "#port = pd.unique(a['port'])\n",
    "#a.columns\n",
    "#pd.unique(a['job'])\n",
    "#a.head()\n",
    "#232746291\n",
    "#missingSchemaids\n",
    "\n",
    "for column in a.columns:\n",
    "    print(column,len(pd.unique(a[column])))\n",
    "    \n",
    "adapters=pd.unique(a['adapter'])\n",
    "hashes=pd.unique(a['hashvalue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.loc[row_indexer,col_indexer] = value \n",
    "b=a[(a['hashvalue'] == hashes[4]) ]\n",
    "#b = a[(a['port'] == port[0]) & (a['le'] == le[2]) ]\n",
    "b['value'] = b['value'].astype(int)\n",
    "#pd.unique(b['hashvalue'])\n",
    "#pd.unique(a['port'] == '8443')\n",
    "#a['port']\n",
    "ax = createLineplot(b,16,10,1.4,title=\"\")\n",
    "label(ax,500,80)\n",
    "#b\n",
    "len(b['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number messages sent by endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = createData_ym(pfall,0,2)\n",
    "createBarplot(md,24,9,3.0,title=\"number messages sent by all endpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = createData_ym(pfall,0,1)\n",
    "createBarplot(md,24,9,3.0,title=\"number messages with errors sent by all endpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details for different CSENDERENDPOINTID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.unique(mdcountsall[1].index.get_level_values(0))\n",
    "TOP=500000\n",
    "result = pfall.groupby(['CSENDERENDPOINTID']).count()\n",
    "data2 = pd.DataFrame()\n",
    "data2['date'] = result.index.get_level_values(0).astype(str)\n",
    "data2['outcome'] =  result['outcome'].astype(int)\n",
    "topsender =  data2[data2['outcome'] > TOP].sort_values('outcome').reset_index()\n",
    "topsender.columns = ['index', 'CSENDERENDPOINTID', 'outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pfall1 = pfall[pfall['CSENDERENDPOINTID'].isin(topsender['date'])]\n",
    "pfall1 = pfall[pfall['CSENDERENDPOINTID']==int(topsender.iloc[7]['CSENDERENDPOINTID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = str(len(topsender)) + \" senders with more than \" + str(TOP) + \" messages (kernel density estimate )\"\n",
    "ax = createKDE(data2,16,8,2,title)\n",
    "label(ax,1,90)\n",
    "#topsender.iloc[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number messages of selected endpoint (Msgs / h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CSENDERENDPOINTID: \" + str(topsender.iloc[7]['CSENDERENDPOINTID']) + \": number messages so far = \" + str(topsender.iloc[7]['outcome']))\n",
    "        \n",
    "\n",
    "def createHeatmapSeries(pfall, months,category, outcome) :\n",
    "    for month in months:\n",
    "        data2 = createData(pfall,month,outcome)\n",
    "        piv = pd.pivot_table(data2, values=\"outcome\",index=[\"hours\"], columns=[\"days\"], fill_value=0)\n",
    "        #titlestring = \"CSENDERENDPOINTID: \" + str(topsender.iloc[7]['CSENDERENDPOINTID']) + \": \"+ category + \" so far = \" + str(topsender.iloc[7]['outcome']) + \" , month: \" + str(month) \n",
    "        titlestring = \"CSENDERENDPOINTID: \" + str(topsender.iloc[7]['CSENDERENDPOINTID']) + \": \"+ category  + \" month: \" + str(month) \n",
    "        sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "        createHeatmap(piv, titlestring)\n",
    "       \n",
    "createHeatmapSeries(pfall1,[10,11,12,1,2,3,4],'messages', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number messages with errors of selected endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createHeatmapSeries(pfall1, [11,12,1,2,3,4],'messages with errors', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection with LSTM Autoencoders (selected SENDERPOINTID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 22, 10\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = createDataframe(pfall1)\n",
    "train, test = getTrainAndTest(df,0.95)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(train[[OUTCOME]])\n",
    "train[OUTCOME] = scaler.transform(train[[OUTCOME]])\n",
    "test[OUTCOME] = scaler.transform(test[[OUTCOME]])\n",
    "\n",
    "# reshape to [samples, time_steps, n_features]\n",
    "\n",
    "X_train, y_train = create_dataset(train[[OUTCOME]], train.close, TIME_STEPS)\n",
    "X_test, y_test = create_dataset(test[[OUTCOME]], test.close, TIME_STEPS)\n",
    "print(X_train.shape)\n",
    "\n",
    "model = initmodel()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    shuffle=False)\n",
    "\n",
    "X_train_pred = model.predict(X_train)\n",
    "train_mae_loss = np.mean(np.abs(X_train_pred - X_train), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_mae_loss, bins=50, kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_df = testScoreDF(model, 1.5)\n",
    "anomalies     = test_score_df[test_score_df.anomaly == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_score_df.index, test_score_df.loss, label='loss')\n",
    "plt.plot(test_score_df.index, test_score_df.threshold, label='threshold')\n",
    "plt.xticks(rotation=25)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#anomalies.head()\n",
    "#anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "  test[TIME_STEPS:].index, \n",
    "  scaler.inverse_transform(test[TIME_STEPS:].close), \n",
    "  label='msg count'\n",
    ");\n",
    "\n",
    "ax = sns.scatterplot(\n",
    "  anomalies.index,\n",
    "  scaler.inverse_transform(anomalies.close),\n",
    "  color=sns.color_palette()[3],\n",
    "  s=152,\n",
    "  label='anomaly'\n",
    ")\n",
    "plt.xticks(rotation=25)\n",
    "plt.legend();\n",
    "\n",
    "label(ax,5,80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
